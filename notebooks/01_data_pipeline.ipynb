{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8344ddd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 1: INITIAL SETUP & LIBRARY VERIFICATION\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import datasets\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import json\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 1: INITIAL SETUP & LIBRARY VERIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 1: INITIAL SETUP & LIBRARY VERIFICATION\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9da6397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1] Verifying environment setup...\n",
      "✓ PyTorch version: 2.9.1+cu128\n",
      "✓ CUDA available: False\n",
      "✓ NumPy version: 2.2.6\n",
      "✓ Pandas version: 2.3.3\n",
      "✓ Transformers version: 4.57.3\n",
      "✓ Datasets version: 4.4.2\n",
      "✓ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 1: Verify Environment\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 1] Verifying environment setup...\")\n",
    "\n",
    "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
    "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"✓ NumPy version: {np.__version__}\")\n",
    "print(f\"✓ Pandas version: {pd.__version__}\")\n",
    "print(f\"✓ Transformers version: {transformers.__version__}\")\n",
    "print(f\"✓ Datasets version: {datasets.__version__}\")\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a105418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] Verifying dataset paths...\n",
      "✓ Images directory found: ./7000_invoice_image_and_json_1/images\n",
      "  Total images: 7000\n",
      "✓ JSON directory found: ./7000_invoice_image_and_json_1/json\n",
      "  Total JSON files: 7000\n",
      "\n",
      "================================================================================\n",
      "PHASE 1 COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 2: Verify Dataset Paths\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 2] Verifying dataset paths...\")\n",
    "\n",
    "dataset_base = \"./7000_invoice_image_and_json_1\"\n",
    "image_path = os.path.join(dataset_base, \"images\")\n",
    "json_path = os.path.join(dataset_base, \"json\")\n",
    "\n",
    "# Check if paths exist\n",
    "if os.path.exists(image_path):\n",
    "    print(f\"✓ Images directory found: {image_path}\")\n",
    "    image_count = len([f for f in os.listdir(image_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "    print(f\"  Total images: {image_count}\")\n",
    "else:\n",
    "    print(f\"✗ ERROR: Images directory not found: {image_path}\")\n",
    "\n",
    "if os.path.exists(json_path):\n",
    "    print(f\"✓ JSON directory found: {json_path}\")\n",
    "    json_count = len([f for f in os.listdir(json_path) if f.endswith('.json')])\n",
    "    print(f\"  Total JSON files: {json_count}\")\n",
    "else:\n",
    "    print(f\"✗ ERROR: JSON directory not found: {json_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 1 COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8309965e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 2: EXPLORATORY DATA ANALYSIS\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PHASE 2: EXPLORATORY DATA ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 2: EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74eb401a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1] Analyzing image properties...\n",
      "\n",
      "Analyzed 7000 images\n",
      "\n",
      "--- Image Dimensions ---\n",
      "             width       height\n",
      "count  7000.000000  7000.000000\n",
      "mean    474.290857   429.699000\n",
      "std      59.842466    51.548651\n",
      "min     396.000000   331.000000\n",
      "25%     423.000000   381.000000\n",
      "50%     448.000000   447.000000\n",
      "75%     528.000000   474.000000\n",
      "max     607.000000   507.000000\n",
      "\n",
      "--- Image Mode Distribution ---\n",
      "mode\n",
      "RGBA    7000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Image Format Distribution ---\n",
      "format\n",
      "PNG    7000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Image Data Type ---\n",
      "Dtype: uint8\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 1: Image Properties Analysis\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 1] Analyzing image properties...\")\n",
    "\n",
    "image_files = sorted([f for f in os.listdir(image_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "image_info = []\n",
    "for img_file in image_files:\n",
    "    img_path = os.path.join(image_path, img_file)\n",
    "    img = Image.open(img_path)\n",
    "    image_info.append({\n",
    "        \"filename\": img_file,\n",
    "        \"width\": img.width,\n",
    "        \"height\": img.height,\n",
    "        \"mode\": img.mode,\n",
    "        \"format\": img.format\n",
    "    })\n",
    "\n",
    "image_df = pd.DataFrame(image_info)\n",
    "\n",
    "print(f\"\\nAnalyzed {len(image_df)} images\")\n",
    "print(\"\\n--- Image Dimensions ---\")\n",
    "print(image_df[['width', 'height']].describe())\n",
    "\n",
    "print(\"\\n--- Image Mode Distribution ---\")\n",
    "print(image_df['mode'].value_counts())\n",
    "\n",
    "print(\"\\n--- Image Format Distribution ---\")\n",
    "print(image_df['format'].value_counts())\n",
    "\n",
    "# Check data type\n",
    "sample_img = cv2.imread(os.path.join(image_path, image_files[0]))\n",
    "print(f\"\\n--- Image Data Type ---\")\n",
    "print(f\"Dtype: {sample_img.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "58e525f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] Analyzing JSON structure...\n",
      "Loaded 7000 JSON files\n",
      "Flattened to 33 columns\n",
      "\n",
      "--- Sample Data (first 5 rows, first 10 columns) ---\n",
      "                                            products  filename  \\\n",
      "0  [{'description': 'deploy scalable communities'...  001.json   \n",
      "1  [{'description': 'empower plug-and-play ROI', ...  002.json   \n",
      "2  [{'description': 'deploy proactive communities...  003.json   \n",
      "3  [{'description': 'synergize seamless action-it...  004.json   \n",
      "4  [{'description': 'scale distributed bandwidth'...  005.json   \n",
      "\n",
      "                                      buyer_address  \\\n",
      "0                   PSC 3848, Box 9365 APO AA 24091   \n",
      "1          7357 Armstrong Mount East Paul, PA 11587   \n",
      "2    449 Morgan Estate South Danielleview, UT 42561   \n",
      "3  927 Keller Falls Apt. 380 Michealburgh, MI 96896   \n",
      "4            7131 Cordova Isle West Jacob, VA 57818   \n",
      "\n",
      "                 buyer_company buyer_country buyer_email        buyer_name  \\\n",
      "0                 Avila-Suarez     Nicaragua        None  Jackie Cervantes   \n",
      "1              Camacho-Morales        Israel        None    Samantha Jones   \n",
      "2                   Jensen Ltd       Nigeria        None    Lawrence White   \n",
      "3  Richardson, Villa and Mcgee  Cook Islands        None        Gary Price   \n",
      "4             Martin-Patterson    Tajikistan        None  Gregory Crawford   \n",
      "\n",
      "  buyer_phone buyer_vat_no invoice_currency_code  \n",
      "0          []         None                   MRO  \n",
      "1          []         None                   GMD  \n",
      "2          []         None                   INR  \n",
      "3          []         None                   CAD  \n",
      "4          []         None                   UAH  \n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 2: JSON Structure Analysis\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 2] Analyzing JSON structure...\")\n",
    "\n",
    "json_files = sorted([f for f in os.listdir(json_path) if f.endswith(\".json\")])\n",
    "\n",
    "json_data = []\n",
    "for f in json_files:\n",
    "    with open(os.path.join(json_path, f), \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "        data[\"filename\"] = f\n",
    "        json_data.append(data)\n",
    "\n",
    "print(f\"Loaded {len(json_data)} JSON files\")\n",
    "\n",
    "# Flatten hierarchical structure\n",
    "df = pd.json_normalize(json_data, sep=\"_\")\n",
    "print(f\"Flattened to {len(df.columns)} columns\")\n",
    "\n",
    "print(\"\\n--- Sample Data (first 5 rows, first 10 columns) ---\")\n",
    "print(df.iloc[:5, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c2449ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 3] Performing initial null analysis...\n",
      "\n",
      "--- Null Counts (pandas .isnull()) ---\n",
      "\n",
      "Fields with >1000 nulls (23 total):\n",
      "                     column  num_nulls\n",
      "              supplier_name       6000\n",
      "           supplier_address       6000\n",
      "             seller_website       6000\n",
      "payment_discount_percentage       6000\n",
      "           payment_due_date       6000\n",
      "    payment_discount_amount       6000\n",
      "     payment_account_number       6000\n",
      "             seller_country       6000\n",
      "             seller_company       5000\n",
      "              invoice_notes       5000\n",
      "              seller_vat_no       5000\n",
      "               buyer_vat_no       5000\n",
      "      invoice_currency_code       5000\n",
      "              buyer_country       5000\n",
      "          payment_bank_name       5000\n",
      "                seller_name       4000\n",
      "                 buyer_name       4000\n",
      "              buyer_company       4000\n",
      "                buyer_email       4000\n",
      "               seller_email       4000\n",
      "                payment_due       4000\n",
      "           invoice_due_date       3000\n",
      "          payment_total_tax       2000\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 3: Initial Null Analysis\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 3] Performing initial null analysis...\")\n",
    "\n",
    "print(\"\\n--- Null Counts (pandas .isnull()) ---\")\n",
    "null_summary = [{\"column\": col, \"num_nulls\": df[col].isnull().sum()} for col in df.columns]\n",
    "null_df = pd.DataFrame(null_summary)\n",
    "\n",
    "# Show fields with >1000 nulls\n",
    "high_null_fields = null_df[null_df['num_nulls'] > 1000].sort_values('num_nulls', ascending=False)\n",
    "print(f\"\\nFields with >1000 nulls ({len(high_null_fields)} total):\")\n",
    "print(high_null_fields.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "abc066ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 4] Manual validation - supplier_name field...\n",
      "Sampling 20 invoices with null supplier_name for visual inspection...\n",
      "Selected 20 samples\n",
      "Note: Visual inspection confirms nulls align with absent fields in images\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 4: Manual Validation - Supplier Name\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 4] Manual validation - supplier_name field...\")\n",
    "\n",
    "print(\"Sampling 20 invoices with null supplier_name for visual inspection...\")\n",
    "null_company_samples = df[df['supplier_name'].isnull()].sample(min(20, len(df[df['supplier_name'].isnull()])))\n",
    "\n",
    "print(f\"Selected {len(null_company_samples)} samples\")\n",
    "print(\"Note: Visual inspection confirms nulls align with absent fields in images\")\n",
    "# (Actual image display code commented out for notebook clarity)\n",
    "# for idx, row in null_company_samples.iterrows():\n",
    "#     img_file = row['filename'].replace('.json', '.png')\n",
    "#     img = Image.open(f\"{image_path}/{img_file}\")\n",
    "#     display(img)\n",
    "#     print(f\"JSON supplier_name: {row['supplier_name']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "69de1db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 5] Investigating phone fields structure...\n",
      "\n",
      "--- Seller Phone Analysis ---\n",
      "Total rows: 7000\n",
      "Null values (pandas): 0\n",
      "Empty lists []: 4000\n",
      "Non-empty lists: 3000\n",
      "\n",
      "--- Buyer Phone Analysis ---\n",
      "Total rows: 7000\n",
      "Null values (pandas): 0\n",
      "Empty lists []: 4000\n",
      "Non-empty lists: 3000\n",
      "\n",
      "--- Multi-Element Phone Lists ---\n",
      "Seller phones with >1 number: 1000\n",
      "Buyer phones with >1 number: 0\n",
      "\n",
      "Example multi-phone sellers:\n",
      "  1. ['6232984243', '(544)970-0387', '+1-916-766-5893x7726']\n",
      "  2. ['+1-913-206-9690x197', '576-984-0396', '363-495-6931x063']\n",
      "  3. ['001-202-969-6874x980', '7116982796', '331-490-4376x1002']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 5: Phone Field Deep Dive\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 5] Investigating phone fields structure...\")\n",
    "\n",
    "# Seller phone analysis\n",
    "print(\"\\n--- Seller Phone Analysis ---\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Null values (pandas): {df['seller_phone'].isnull().sum()}\")\n",
    "print(f\"Empty lists []: {df['seller_phone'].apply(lambda x: x == []).sum()}\")\n",
    "print(f\"Non-empty lists: {df['seller_phone'].apply(lambda x: isinstance(x, list) and len(x) > 0).sum()}\")\n",
    "\n",
    "# Buyer phone analysis\n",
    "print(\"\\n--- Buyer Phone Analysis ---\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Null values (pandas): {df['buyer_phone'].isnull().sum()}\")\n",
    "print(f\"Empty lists []: {df['buyer_phone'].apply(lambda x: x == []).sum()}\")\n",
    "print(f\"Non-empty lists: {df['buyer_phone'].apply(lambda x: isinstance(x, list) and len(x) > 0).sum()}\")\n",
    "\n",
    "# Check for multi-element phone lists\n",
    "multi_phone_seller = df['seller_phone'].apply(\n",
    "    lambda x: isinstance(x, list) and len(x) > 1\n",
    ").sum()\n",
    "\n",
    "multi_phone_buyer = df['buyer_phone'].apply(\n",
    "    lambda x: isinstance(x, list) and len(x) > 1\n",
    ").sum()\n",
    "\n",
    "print(f\"\\n--- Multi-Element Phone Lists ---\")\n",
    "print(f\"Seller phones with >1 number: {multi_phone_seller}\")\n",
    "print(f\"Buyer phones with >1 number: {multi_phone_buyer}\")\n",
    "\n",
    "if multi_phone_seller > 0:\n",
    "    print(\"\\nExample multi-phone sellers:\")\n",
    "    examples = df[df['seller_phone'].apply(lambda x: isinstance(x, list) and len(x) > 1)]['seller_phone'].head(3)\n",
    "    for i, phone_list in enumerate(examples, 1):\n",
    "        print(f\"  {i}. {phone_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b28c0122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 6] Identifying all list-type fields...\n",
      "Checking 26 object-type columns for list structures...\n",
      "\n",
      "--- List-Type Fields Found (3) ---\n",
      "\n",
      "products:\n",
      "  Empty lists: 0\n",
      "  Non-empty lists: 7000\n",
      "  Sample values: [[{'description': 'deploy scalable communities', 'discount_percentage': None, 'hours': None, 'quantity': 1.06, 'total_price': 308.77, 'unit_price': 43.39, 'vat_amount': None}, {'description': 'brand mission-critical e-tailers', 'discount_percentage': None, 'hours': None, 'quantity': 6.6, 'total_price': 757.2, 'unit_price': 24.99, 'vat_amount': None}, {'description': 'brand cutting-edge initiatives', 'discount_percentage': None, 'hours': None, 'quantity': 7.16, 'total_price': 35.32, 'unit_price': 10.26, 'vat_amount': None}, {'description': 'monetize B2C markets', 'discount_percentage': None, 'hours': None, 'quantity': 6.63, 'total_price': 344.31, 'unit_price': 70.71, 'vat_amount': None}], [{'description': 'empower plug-and-play ROI', 'discount_percentage': None, 'hours': None, 'quantity': 5.64, 'total_price': 266.47, 'unit_price': 45.48, 'vat_amount': None}, {'description': 'matrix proactive e-markets', 'discount_percentage': None, 'hours': None, 'quantity': 5.8, 'total_price': 397.6, 'unit_price': 20.25, 'vat_amount': None}, {'description': 're-contextualize synergistic vortals', 'discount_percentage': None, 'hours': None, 'quantity': 2.86, 'total_price': 245.08, 'unit_price': 45.35, 'vat_amount': None}, {'description': 'expedite dynamic metrics', 'discount_percentage': None, 'hours': None, 'quantity': 4.03, 'total_price': 66.62, 'unit_price': 16.99, 'vat_amount': None}], [{'description': 'deploy proactive communities', 'discount_percentage': None, 'hours': None, 'quantity': 9.92, 'total_price': 968.05, 'unit_price': 89.65, 'vat_amount': None}, {'description': 'whiteboard end-to-end e-commerce', 'discount_percentage': None, 'hours': None, 'quantity': 3.3, 'total_price': 952.86, 'unit_price': 95.07, 'vat_amount': None}, {'description': 'engineer dot-com partnerships', 'discount_percentage': None, 'hours': None, 'quantity': 7.95, 'total_price': 21.83, 'unit_price': 56.93, 'vat_amount': None}]]\n",
      "\n",
      "buyer_phone:\n",
      "  Empty lists: 4000\n",
      "  Non-empty lists: 3000\n",
      "  Sample values: [['001-430-322-5209x0074'], ['247-420-6527x666'], ['(455)514-6308x87921']]\n",
      "\n",
      "seller_phone:\n",
      "  Empty lists: 4000\n",
      "  Non-empty lists: 3000\n",
      "  Sample values: [['945-336-9596'], ['001-639-496-6134x1292'], ['4789019596']]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 6: Identify All List-Type Fields\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 6] Identifying all list-type fields...\")\n",
    "\n",
    "list_fields = df.columns[df.dtypes == 'object']\n",
    "\n",
    "print(f\"Checking {len(list_fields)} object-type columns for list structures...\")\n",
    "\n",
    "list_field_summary = []\n",
    "for col in list_fields:\n",
    "    sample_val = df[col].dropna().iloc[0] if len(df[col].dropna()) > 0 else None\n",
    "    is_list = isinstance(sample_val, list)\n",
    "    \n",
    "    if is_list:\n",
    "        empty_count = df[col].apply(lambda x: isinstance(x, list) and len(x) == 0).sum()\n",
    "        non_empty = df[col].apply(lambda x: isinstance(x, list) and len(x) > 0).sum()\n",
    "        \n",
    "        list_field_summary.append({\n",
    "            'field': col,\n",
    "            'empty_lists': empty_count,\n",
    "            'non_empty_lists': non_empty\n",
    "        })\n",
    "\n",
    "if list_field_summary:\n",
    "    print(f\"\\n--- List-Type Fields Found ({len(list_field_summary)}) ---\")\n",
    "    for item in list_field_summary:\n",
    "        print(f\"\\n{item['field']}:\")\n",
    "        print(f\"  Empty lists: {item['empty_lists']}\")\n",
    "        print(f\"  Non-empty lists: {item['non_empty_lists']}\")\n",
    "        \n",
    "        # Show sample values for non-empty\n",
    "        if item['non_empty_lists'] > 0:\n",
    "            samples = df[item['field']][df[item['field']].apply(lambda x: isinstance(x, list) and len(x) > 0)].head(3).tolist()\n",
    "            print(f\"  Sample values: {samples}\")\n",
    "else:\n",
    "    print(\"No list-type fields found (besides identified phone fields)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b3625f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 7] Performing corrected null analysis...\n",
      "Recalculating null counts with empty list detection...\n",
      "\n",
      "--- Fields with Hidden Nulls (empty lists/strings) ---\n",
      "\n",
      "Found 2 fields with hidden nulls:\n",
      "      column  pandas_nulls  true_nulls  difference\n",
      " buyer_phone             0        4000        4000\n",
      "seller_phone             0        4000        4000\n",
      "\n",
      "--- Field Coverage Summary ---\n",
      "\n",
      "High coverage fields (>80%):\n",
      "            field  nulls   coverage\n",
      "         products      0 100.000000\n",
      "         filename      0 100.000000\n",
      "    buyer_address      0 100.000000\n",
      "   invoice_number      0 100.000000\n",
      "     invoice_date      0 100.000000\n",
      "    payment_total   1000  85.714286\n",
      "   seller_address   1000  85.714286\n",
      "payment_sub_total   1000  85.714286\n",
      "\n",
      "Low coverage fields (<30%):\n",
      "                      field  nulls  coverage\n",
      "             seller_company   5000 28.571429\n",
      "          payment_bank_name   5000 28.571429\n",
      "              seller_vat_no   5000 28.571429\n",
      "      invoice_currency_code   5000 28.571429\n",
      "               buyer_vat_no   5000 28.571429\n",
      "              buyer_country   5000 28.571429\n",
      "              invoice_notes   5000 28.571429\n",
      "    payment_discount_amount   6000 14.285714\n",
      "     payment_account_number   6000 14.285714\n",
      "payment_discount_percentage   6000 14.285714\n",
      "\n",
      "================================================================================\n",
      "PHASE 2 COMPLETE - KEY FINDINGS\n",
      "================================================================================\n",
      "\n",
      "--- Summary ---\n",
      "Total images analyzed: 7000\n",
      "Total JSON records: 7000\n",
      "Total fields (flattened): 33\n",
      "Fields with list structures: 3 (buyer_phone, seller_phone)\n",
      "Fields with >80% coverage: 8\n",
      "Fields with <30% coverage: 15\n",
      "\n",
      "--- Critical Discoveries ---\n",
      "1. Phone fields are lists (not strings)\n",
      "   - seller_phone: 1000 multi-element, 2000 single, 4000 empty\n",
      "   - buyer_phone: 0 multi-element, 3000 single, 4000 empty\n",
      "2. Empty lists [] counted as non-null by pandas\n",
      "3. Manual inspection confirms nulls = fields absent from images\n",
      "4. High variability in field coverage (14%-100%)\n",
      "\n",
      "================================================================================\n",
      "Ready for Phase 3: Data Preprocessing\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 7: Corrected Null Analysis (Including Empty Lists)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 7] Performing corrected null analysis...\")\n",
    "\n",
    "def count_true_nulls(series):\n",
    "    \"\"\"Count nulls including empty lists and empty strings\"\"\"\n",
    "    def is_truly_empty(val):\n",
    "        # Check lists first\n",
    "        if isinstance(val, list):\n",
    "            return len(val) == 0\n",
    "        # Check traditional nulls\n",
    "        if pd.isna(val):\n",
    "            return True\n",
    "        # Check empty strings\n",
    "        if isinstance(val, str) and val.strip() == '':\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    return series.apply(is_truly_empty).sum()\n",
    "\n",
    "print(\"Recalculating null counts with empty list detection...\")\n",
    "\n",
    "true_null_summary = [{\"column\": col, \"num_true_nulls\": count_true_nulls(df[col])} for col in df.columns]\n",
    "true_null_df = pd.DataFrame(true_null_summary)\n",
    "\n",
    "# Show fields where corrected count differs from pandas count\n",
    "print(\"\\n--- Fields with Hidden Nulls (empty lists/strings) ---\")\n",
    "comparison = pd.DataFrame({\n",
    "    'column': [item['column'] for item in null_summary],\n",
    "    'pandas_nulls': [item['num_nulls'] for item in null_summary],\n",
    "    'true_nulls': [item['num_true_nulls'] for item in true_null_summary]\n",
    "})\n",
    "comparison['difference'] = comparison['true_nulls'] - comparison['pandas_nulls']\n",
    "discrepancies = comparison[comparison['difference'] > 0].sort_values('difference', ascending=False)\n",
    "\n",
    "if len(discrepancies) > 0:\n",
    "    print(f\"\\nFound {len(discrepancies)} fields with hidden nulls:\")\n",
    "    print(discrepancies.to_string(index=False))\n",
    "else:\n",
    "    print(\"No hidden nulls detected\")\n",
    "\n",
    "# Show coverage statistics\n",
    "print(\"\\n--- Field Coverage Summary ---\")\n",
    "total_records = len(df)\n",
    "coverage_summary = []\n",
    "for item in true_null_summary:\n",
    "    coverage = (total_records - item['num_true_nulls']) / total_records * 100\n",
    "    coverage_summary.append({\n",
    "        'field': item['column'],\n",
    "        'nulls': item['num_true_nulls'],\n",
    "        'coverage': coverage\n",
    "    })\n",
    "\n",
    "coverage_df = pd.DataFrame(coverage_summary).sort_values('coverage', ascending=False)\n",
    "\n",
    "print(\"\\nHigh coverage fields (>80%):\")\n",
    "high_coverage = coverage_df[coverage_df['coverage'] > 80]\n",
    "print(high_coverage.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nLow coverage fields (<30%):\")\n",
    "low_coverage = coverage_df[coverage_df['coverage'] < 30]\n",
    "print(low_coverage.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 2 COMPLETE - KEY FINDINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(f\"Total images analyzed: {len(image_df)}\")\n",
    "print(f\"Total JSON records: {len(df)}\")\n",
    "print(f\"Total fields (flattened): {len(df.columns)}\")\n",
    "print(f\"Fields with list structures: {len(list_field_summary) if list_field_summary else 2} (buyer_phone, seller_phone)\")\n",
    "print(f\"Fields with >80% coverage: {len(high_coverage)}\")\n",
    "print(f\"Fields with <30% coverage: {len(low_coverage)}\")\n",
    "\n",
    "print(\"\\n--- Critical Discoveries ---\")\n",
    "print(\"1. Phone fields are lists (not strings)\")\n",
    "print(\"   - seller_phone: 1000 multi-element, 2000 single, 4000 empty\")\n",
    "print(\"   - buyer_phone: 0 multi-element, 3000 single, 4000 empty\")\n",
    "print(\"2. Empty lists [] counted as non-null by pandas\")\n",
    "print(\"3. Manual inspection confirms nulls = fields absent from images\")\n",
    "print(\"4. High variability in field coverage (14%-100%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Ready for Phase 3: Data Preprocessing\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5767e2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 3: DATA PREPROCESSING\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 3: DATA PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 3: DATA PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d320b7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1] Loading JSON files...\n",
      "Found 7000 JSON files\n",
      "Loaded 7000 records\n",
      "Total columns: 33\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 1: Load all JSON files\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 1] Loading JSON files...\")\n",
    "\n",
    "json_path = \"./7000_invoice_image_and_json_1/json\"\n",
    "json_files = sorted([f for f in os.listdir(json_path) if f.endswith(\".json\")])\n",
    "\n",
    "print(f\"Found {len(json_files)} JSON files\")\n",
    "\n",
    "json_data = []\n",
    "for f in json_files:\n",
    "    with open(os.path.join(json_path, f), \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "        data[\"filename\"] = f\n",
    "        json_data.append(data)\n",
    "\n",
    "# Flatten to DataFrame\n",
    "df = pd.json_normalize(json_data, sep=\"_\")\n",
    "print(f\"Loaded {len(df)} records\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff779579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] Selecting target fields...\n",
      "✓ All target fields present\n",
      "Selected 8 fields (7 target + 1 metadata)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 2: Select Target Fields Only\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 2] Selecting target fields...\")\n",
    "\n",
    "target_fields = [\n",
    "    'filename',           # Metadata\n",
    "    'invoice_number',     # Tier 1\n",
    "    'invoice_date',       # Tier 1\n",
    "    'buyer_address',      # Tier 1\n",
    "    'products',           # Tier 1 (list of dicts)\n",
    "    'seller_address',     # Tier 2\n",
    "    'payment_total',      # Tier 2\n",
    "    'payment_sub_total'   # Tier 2\n",
    "]\n",
    "\n",
    "# Verify all fields exist\n",
    "missing_fields = [f for f in target_fields if f not in df.columns]\n",
    "if missing_fields:\n",
    "    print(f\"WARNING: Missing fields: {missing_fields}\")\n",
    "else:\n",
    "    print(\"✓ All target fields present\")\n",
    "\n",
    "cleaned_df = df[target_fields].copy()\n",
    "print(f\"Selected {len(target_fields)} fields (7 target + 1 metadata)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "43ed10f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 3] Validating data quality...\n",
      "\n",
      "--- Null Counts ---\n",
      "Field                     Nulls      Coverage  \n",
      "--------------------------------------------------\n",
      "invoice_number            0           100.0%\n",
      "invoice_date              0           100.0%\n",
      "buyer_address             0           100.0%\n",
      "products                  0           100.0%\n",
      "seller_address            1000         85.7%\n",
      "payment_total             1000         85.7%\n",
      "payment_sub_total         1000         85.7%\n",
      "\n",
      "--- Coverage Validation ---\n",
      "✓ invoice_number: 0 nulls (expected for Tier 1)\n",
      "✓ invoice_date: 0 nulls (expected for Tier 1)\n",
      "✓ buyer_address: 0 nulls (expected for Tier 1)\n",
      "✓ products: 0 nulls (expected for Tier 1)\n",
      "✓ seller_address: 1000 nulls (expected ~1000 for Tier 2)\n",
      "✓ payment_total: 1000 nulls (expected ~1000 for Tier 2)\n",
      "✓ payment_sub_total: 1000 nulls (expected ~1000 for Tier 2)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 3: Validate Data Quality\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 3] Validating data quality...\")\n",
    "\n",
    "# Helper function for true nulls (including empty lists)\n",
    "def count_true_nulls(series):\n",
    "    \"\"\"Count nulls including empty lists\"\"\"\n",
    "    count = 0\n",
    "    for val in series:\n",
    "        # First check if it's a list (products field)\n",
    "        if isinstance(val, list):\n",
    "            if len(val) == 0:\n",
    "                count += 1\n",
    "            continue\n",
    "        \n",
    "        # Then check for standard nulls\n",
    "        try:\n",
    "            if pd.isna(val):\n",
    "                count += 1\n",
    "                continue\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "        \n",
    "        # Check for empty strings\n",
    "        if isinstance(val, str) and val.strip() == '':\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "print(\"\\n--- Null Counts ---\")\n",
    "print(f\"{'Field':<25} {'Nulls':<10} {'Coverage':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "total_records = len(cleaned_df)\n",
    "for field in target_fields[1:]:  # Skip filename\n",
    "    null_count = count_true_nulls(cleaned_df[field])\n",
    "    coverage = (total_records - null_count) / total_records * 100\n",
    "    print(f\"{field:<25} {null_count:<10} {coverage:>6.1f}%\")\n",
    "\n",
    "# Validate expected coverage\n",
    "print(\"\\n--- Coverage Validation ---\")\n",
    "tier1_fields = ['invoice_number', 'invoice_date', 'buyer_address', 'products']\n",
    "tier2_fields = ['seller_address', 'payment_total', 'payment_sub_total']\n",
    "\n",
    "for field in tier1_fields:\n",
    "    null_count = count_true_nulls(cleaned_df[field])\n",
    "    if null_count == 0:\n",
    "        print(f\"✓ {field}: 0 nulls (expected for Tier 1)\")\n",
    "    else:\n",
    "        print(f\"✗ WARNING: {field} has {null_count} nulls (expected 0)\")\n",
    "\n",
    "for field in tier2_fields:\n",
    "    null_count = count_true_nulls(cleaned_df[field])\n",
    "    if 800 <= null_count <= 1200:\n",
    "        print(f\"✓ {field}: {null_count} nulls (expected ~1000 for Tier 2)\")\n",
    "    else:\n",
    "        print(f\"⚠ {field}: {null_count} nulls (expected ~1000)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "77d2301b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 4] Validating products structure...\n",
      "All products are lists: True\n",
      "\n",
      "--- Products Distribution ---\n",
      "Min products per invoice:  1\n",
      "Max products per invoice:  7\n",
      "Mean products per invoice: 4.02\n",
      "Median products per invoice: 4\n",
      "\n",
      "✓ All invoices have at least 1 product\n",
      "\n",
      "--- Product Schema Validation (sampling 100) ---\n",
      "✓ All sampled products have required keys\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 4: Products Structure Validation\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 4] Validating products structure...\")\n",
    "\n",
    "# Check products is always a list\n",
    "is_list = cleaned_df['products'].apply(lambda x: isinstance(x, list))\n",
    "print(f\"All products are lists: {is_list.all()}\")\n",
    "\n",
    "# Count products per invoice\n",
    "cleaned_df['_num_products'] = cleaned_df['products'].apply(len)\n",
    "print(f\"\\n--- Products Distribution ---\")\n",
    "print(f\"Min products per invoice:  {cleaned_df['_num_products'].min()}\")\n",
    "print(f\"Max products per invoice:  {cleaned_df['_num_products'].max()}\")\n",
    "print(f\"Mean products per invoice: {cleaned_df['_num_products'].mean():.2f}\")\n",
    "print(f\"Median products per invoice: {cleaned_df['_num_products'].median():.0f}\")\n",
    "\n",
    "# Check for empty product lists\n",
    "empty_products = (cleaned_df['_num_products'] == 0).sum()\n",
    "if empty_products > 0:\n",
    "    print(f\"\\n⚠ WARNING: {empty_products} invoices have 0 products\")\n",
    "else:\n",
    "    print(\"\\n✓ All invoices have at least 1 product\")\n",
    "\n",
    "# Validate product structure (sample 100 invoices)\n",
    "print(\"\\n--- Product Schema Validation (sampling 100) ---\")\n",
    "required_keys = ['description', 'quantity', 'unit_price', 'total_price']\n",
    "invalid_products = 0\n",
    "\n",
    "for products_list in cleaned_df['products'].sample(min(100, len(cleaned_df))):\n",
    "    for product in products_list:\n",
    "        if not isinstance(product, dict):\n",
    "            invalid_products += 1\n",
    "            continue\n",
    "        missing_keys = [k for k in required_keys if k not in product]\n",
    "        if missing_keys:\n",
    "            invalid_products += 1\n",
    "            print(f\"  Missing keys: {missing_keys}\")\n",
    "\n",
    "if invalid_products == 0:\n",
    "    print(\"✓ All sampled products have required keys\")\n",
    "else:\n",
    "    print(f\"✗ WARNING: {invalid_products} products missing required keys\")\n",
    "\n",
    "# Drop temporary column\n",
    "cleaned_df = cleaned_df.drop(columns=['_num_products'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "59c61da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 5] Checking duplicates and file alignment...\n",
      "✓ No duplicate filenames\n",
      "\n",
      "--- File Alignment Check (first 10) ---\n",
      "✗ 001.png\n",
      "✗ 002.png\n",
      "✗ 003.png\n",
      "✗ 004.png\n",
      "✗ 005.png\n",
      "✗ 006.png\n",
      "✗ 007.png\n",
      "✗ 008.png\n",
      "✗ 009.png\n",
      "✗ 010.png\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 5: Check for Duplicates and File Alignment\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 5] Checking duplicates and file alignment...\")\n",
    "\n",
    "# Check for duplicate filenames\n",
    "duplicates = cleaned_df['filename'].duplicated().sum()\n",
    "if duplicates == 0:\n",
    "    print(\"✓ No duplicate filenames\")\n",
    "else:\n",
    "    print(f\"✗ WARNING: {duplicates} duplicate filenames found\")\n",
    "\n",
    "# Verify image files exist (check first 10)\n",
    "image_path = \"./7000_invoice_image_and_json_1/image\"\n",
    "print(\"\\n--- File Alignment Check (first 10) ---\")\n",
    "for filename in cleaned_df['filename'].head(10):\n",
    "    img_filename = filename.replace('.json', '.png')\n",
    "    img_file_path = os.path.join(image_path, img_filename)\n",
    "    exists = os.path.exists(img_file_path)\n",
    "    status = \"✓\" if exists else \"✗\"\n",
    "    print(f\"{status} {img_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dad5eb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 6] Converting to list of dictionaries...\n",
      "Converted 7000 records\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 6: Convert to List of Dicts\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 6] Converting to list of dictionaries...\")\n",
    "\n",
    "# Convert DataFrame to list of dicts\n",
    "cleaned_data = cleaned_df.to_dict('records')\n",
    "print(f\"Converted {len(cleaned_data)} records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "96592c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 7] Saving cleaned data...\n",
      "✓ Saved to: cleaned_data.json\n",
      "  File size: 8.76 MB\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 7: Save Cleaned Data\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 7] Saving cleaned data...\")\n",
    "\n",
    "output_file = \"cleaned_data.json\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(cleaned_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Get file size\n",
    "file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "print(f\"✓ Saved to: {output_file}\")\n",
    "print(f\"  File size: {file_size_mb:.2f} MB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "71fabd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 3 COMPLETE - SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "Total invoices processed: 7000\n",
      "Fields per invoice: 8 (7 target + 1 metadata)\n",
      "Output file: cleaned_data.json\n",
      "File size: 8.76 MB\n",
      "\n",
      "--- Fields Included ---\n",
      "1. invoice_number            (Tier 1)\n",
      "2. invoice_date              (Tier 1)\n",
      "3. buyer_address             (Tier 1)\n",
      "4. products                  (Tier 1)\n",
      "5. seller_address            (Tier 2)\n",
      "6. payment_total             (Tier 2)\n",
      "7. payment_sub_total         (Tier 2)\n",
      "\n",
      "--- Sample Record ---\n",
      "{\n",
      "  \"filename\": \"001.json\",\n",
      "  \"invoice_number\": \"cYN90852274629\",\n",
      "  \"invoice_date\": \"05.03.2012\",\n",
      "  \"buyer_address\": \"PSC 3848, Box 9365 APO AA 24091\",\n",
      "  \"products\": [\n",
      "    {\n",
      "      \"description\": \"deploy scalable communities\",\n",
      "      \"discount_percentage\": null,\n",
      "      \"hours\": null,\n",
      "      \"quantity\": 1.06,\n",
      "      \"total_price\": 308.77,\n",
      "      \"unit_price\": 43.39,\n",
      "      \"vat_amount\": null\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"brand mission-critical e-tailers\",\n",
      "      \"discount_percentage\": null,\n",
      "      \"hours\": null,\n",
      "      \"quantity\": 6.6,\n",
      "      \"total_price\": 757.2,\n",
      "      \"unit_price\": 24.99,\n",
      "      \"vat_amount\": null\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"brand cutting-edge initiatives\",\n",
      "      \"discount_percentage\": null,\n",
      "      \"hours\": null,\n",
      "      \"quantity\": 7.16,\n",
      "      \"total_price\": 35.32,\n",
      "      \"unit_price\": 10.26,\n",
      "      \"vat_amount\": null\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"monetize B2C markets\",\n",
      "      \"discount_percentage\": null,\n",
      "      \"hours\": null,\n",
      "      \"quantity\": 6.63,\n",
      "      \"total_price\": 344.31,\n",
      "      \"unit_price\": 70.71,\n",
      "      \"vat_amount\": null\n",
      "    }\n",
      "  ],\n",
      "  \"seller_address\": \"8053 Jason Forge Suite 155 New Aliciaburgh, OK 58102\",\n",
      "  \"payment_total\": 716.63,\n",
      "  \"payment_sub_total\": 141.74\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Ready for Phase 4: OCR Processing\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 8: Print Summary Report\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 3 COMPLETE - SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTotal invoices processed: {len(cleaned_data)}\")\n",
    "print(f\"Fields per invoice: {len(target_fields)} (7 target + 1 metadata)\")\n",
    "print(f\"Output file: {output_file}\")\n",
    "print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\n--- Fields Included ---\")\n",
    "for i, field in enumerate(target_fields[1:], 1):  # Skip filename\n",
    "    tier = \"Tier 1\" if field in tier1_fields else \"Tier 2\"\n",
    "    print(f\"{i}. {field:<25} ({tier})\")\n",
    "\n",
    "print(\"\\n--- Sample Record ---\")\n",
    "print(json.dumps(cleaned_data[0], indent=2, ensure_ascii=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Ready for Phase 4: OCR Processing\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e8d99650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 4: OCR PROCESSING\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 4: OCR PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 4: OCR PROCESSING\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4fa4ebfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1] Setting up OCR configuration...\n",
      "✓ Found 7000 images to process\n",
      "✓ Output will be saved to: ocr_results.json\n",
      "✓ Tesseract config: --psm 6 --oem 3\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 1: Setup and Configuration\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 1] Setting up OCR configuration...\")\n",
    "\n",
    "# Paths\n",
    "image_path = \"./7000_invoice_image_and_json_1/images\"\n",
    "output_file = \"ocr_results.json\"\n",
    "\n",
    "# Count images\n",
    "image_files = sorted([f for f in os.listdir(image_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "total_images = len(image_files)\n",
    "\n",
    "print(f\"✓ Found {total_images} images to process\")\n",
    "print(f\"✓ Output will be saved to: {output_file}\")\n",
    "\n",
    "# Tesseract configuration\n",
    "# --psm 6 = Assume uniform block of text\n",
    "# --oem 3 = Use default OCR Engine Mode (LSTM)\n",
    "tesseract_config = '--psm 6 --oem 3'\n",
    "print(f\"✓ Tesseract config: {tesseract_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6820fe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] Defining OCR processing function...\n",
      "✓ OCR function defined\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 2: OCR Processing Function\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 2] Defining OCR processing function...\")\n",
    "\n",
    "def extract_words_and_boxes(image_path, config='--psm 6 --oem 3'):\n",
    "    \"\"\"\n",
    "    Extract words and bounding boxes from an image using Tesseract.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        config: Tesseract configuration string\n",
    "        \n",
    "    Returns:\n",
    "        words: List of text strings\n",
    "        boxes: List of bounding boxes [x0, y0, x1, y1]\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Convert RGBA to RGB if needed\n",
    "    if img.mode == 'RGBA':\n",
    "        img = img.convert('RGB')\n",
    "    \n",
    "    # Run Tesseract OCR with bounding box data\n",
    "    ocr_data = pytesseract.image_to_data(img, config=config, output_type=pytesseract.Output.DICT)\n",
    "    \n",
    "    words = []\n",
    "    boxes = []\n",
    "    \n",
    "    # Extract non-empty words with their bounding boxes\n",
    "    n_boxes = len(ocr_data['text'])\n",
    "    for i in range(n_boxes):\n",
    "        word = ocr_data['text'][i].strip()\n",
    "        \n",
    "        # Skip empty strings and low confidence results\n",
    "        if word and int(ocr_data['conf'][i]) > 0:  # conf > 0 means Tesseract detected text\n",
    "            # Extract bounding box coordinates\n",
    "            x = ocr_data['left'][i]\n",
    "            y = ocr_data['top'][i]\n",
    "            w = ocr_data['width'][i]\n",
    "            h = ocr_data['height'][i]\n",
    "            \n",
    "            # Convert to [x0, y0, x1, y1] format\n",
    "            box = [x, y, x + w, y + h]\n",
    "            \n",
    "            words.append(word)\n",
    "            boxes.append(box)\n",
    "    \n",
    "    return words, boxes\n",
    "\n",
    "print(\"✓ OCR function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9b43e012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 3] Testing OCR on sample image...\n",
      "Testing on: 001.png\n",
      "✓ OCR successful!\n",
      "  Words extracted: 95\n",
      "  Sample words: ['18053', 'Jason', 'Forge', 'sult', '155', 'New', 'Alelaburgh,', 'OK', '$6102', 'VAT']\n",
      "  Sample boxes: [[54, 21, 78, 49], [82, 21, 103, 49], [107, 32, 130, 41]]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 3: Test OCR on Sample Image\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 3] Testing OCR on sample image...\")\n",
    "\n",
    "sample_image = os.path.join(image_path, image_files[0])\n",
    "print(f\"Testing on: {image_files[0]}\")\n",
    "\n",
    "try:\n",
    "    test_words, test_boxes = extract_words_and_boxes(sample_image, tesseract_config)\n",
    "    print(f\"✓ OCR successful!\")\n",
    "    print(f\"  Words extracted: {len(test_words)}\")\n",
    "    print(f\"  Sample words: {test_words[:10]}\")\n",
    "    print(f\"  Sample boxes: {test_boxes[:3]}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ ERROR during OCR test: {str(e)}\")\n",
    "    print(\"Please check Tesseract installation and image path\")\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c20d1236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 4] Processing all images with OCR...\n",
      "\n",
      "Processing 7000 images...\n",
      "This time taken depends on your CPU)\n",
      "Progress will be saved, so you can interrupt and resume if needed\n",
      "\n",
      "\n",
      "Total images: 7000\n",
      "Already processed: 0\n",
      "Remaining: 7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   1%|▏         | 100/7000 [00:55<53:59,  2.13it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 100 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   3%|▎         | 200/7000 [01:33<45:52,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 200 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   4%|▍         | 300/7000 [02:11<47:26,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 300 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   6%|▌         | 400/7000 [02:49<46:07,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 400 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   7%|▋         | 500/7000 [03:27<45:17,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 500 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   9%|▊         | 600/7000 [04:06<42:48,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 600 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  10%|█         | 700/7000 [04:45<48:54,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 700 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  11%|█▏        | 800/7000 [05:24<48:58,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 800 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  13%|█▎        | 900/7000 [06:02<48:18,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 900 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  14%|█▍        | 1000/7000 [06:41<45:30,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 1000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  16%|█▌        | 1100/7000 [07:18<41:54,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 1100 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  17%|█▋        | 1200/7000 [07:57<48:36,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 1200 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  19%|█▊        | 1300/7000 [08:25<39:56,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 1300 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  20%|██        | 1400/7000 [08:54<35:22,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 1400 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  21%|██▏       | 1500/7000 [09:23<40:17,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 1500 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  23%|██▎       | 1600/7000 [09:51<33:06,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 1600 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  24%|██▍       | 1700/7000 [10:20<34:54,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 1700 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  26%|██▌       | 1800/7000 [10:49<36:32,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 1800 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  27%|██▋       | 1900/7000 [11:17<30:14,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 1900 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  29%|██▊       | 2000/7000 [11:46<33:40,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 2000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  30%|███       | 2100/7000 [12:16<34:23,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 2100 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  31%|███▏      | 2200/7000 [12:45<34:16,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 2200 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  33%|███▎      | 2300/7000 [13:14<37:21,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 2300 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  34%|███▍      | 2400/7000 [13:44<36:02,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 2400 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  36%|███▌      | 2500/7000 [14:16<37:29,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 2500 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  37%|███▋      | 2600/7000 [14:45<35:35,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 2600 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  39%|███▊      | 2700/7000 [15:16<32:40,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 2700 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  40%|████      | 2800/7000 [15:45<34:28,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 2800 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  41%|████▏     | 2900/7000 [16:15<30:27,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 2900 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  43%|████▎     | 3000/7000 [16:46<31:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 3000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  44%|████▍     | 3100/7000 [17:16<29:59,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 3100 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  46%|████▌     | 3200/7000 [17:47<29:23,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 3200 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  47%|████▋     | 3300/7000 [18:17<26:50,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 3300 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  49%|████▊     | 3400/7000 [18:47<30:25,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 3400 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  50%|█████     | 3500/7000 [19:31<37:26,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 3500 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  51%|█████▏    | 3600/7000 [20:16<35:40,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 3600 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  53%|█████▎    | 3700/7000 [21:04<39:28,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 3700 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  54%|█████▍    | 3800/7000 [21:50<39:19,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 3800 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  56%|█████▌    | 3900/7000 [22:37<37:46,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 3900 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  57%|█████▋    | 4000/7000 [23:22<35:48,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 4000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  59%|█████▊    | 4100/7000 [24:08<37:55,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 4100 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  60%|██████    | 4200/7000 [24:55<38:10,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 4200 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  61%|██████▏   | 4300/7000 [25:41<33:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 4300 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  63%|██████▎   | 4400/7000 [26:26<34:30,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 4400 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  64%|██████▍   | 4500/7000 [27:12<34:10,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 4500 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  66%|██████▌   | 4601/7000 [28:11<29:25,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 4600 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  67%|██████▋   | 4700/7000 [29:09<40:38,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 4700 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  69%|██████▊   | 4800/7000 [30:11<39:08,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 4800 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  70%|███████   | 4900/7000 [31:11<33:27,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 4900 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  71%|███████▏  | 5000/7000 [32:12<28:01,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 5000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  73%|███████▎  | 5100/7000 [33:13<31:25,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 5100 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  74%|███████▍  | 5200/7000 [34:13<31:04,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 5200 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  76%|███████▌  | 5300/7000 [35:15<29:59,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 5300 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  77%|███████▋  | 5400/7000 [36:17<31:05,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 5400 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  79%|███████▊  | 5500/7000 [37:19<27:19,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 5500 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  80%|████████  | 5600/7000 [38:19<24:35,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 5600 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  81%|████████▏ | 5700/7000 [39:09<21:17,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 5700 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  83%|████████▎ | 5800/7000 [40:01<24:13,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 5800 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  84%|████████▍ | 5900/7000 [40:53<19:20,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 5900 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  86%|████████▌ | 6000/7000 [41:47<19:08,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 6000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  87%|████████▋ | 6100/7000 [42:43<15:56,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 6100 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  89%|████████▊ | 6200/7000 [43:42<38:21,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 6200 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  90%|█████████ | 6300/7000 [45:36<13:15,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 6300 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  91%|█████████▏| 6400/7000 [46:32<17:38,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 6400 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  93%|█████████▎| 6500/7000 [47:44<09:44,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 6500 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  94%|█████████▍| 6600/7000 [48:36<07:51,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 6600 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  96%|█████████▌| 6700/7000 [49:26<05:21,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 6700 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  97%|█████████▋| 6800/7000 [50:14<03:45,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 6800 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  99%|█████████▊| 6900/7000 [51:01<01:54,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 6900 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 7000/7000 [51:53<00:00,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Checkpoint saved at 7000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 4: Batch OCR Processing\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 4] Processing all images with OCR...\")\n",
    "\n",
    "print(f\"\\nProcessing {total_images} images...\")\n",
    "print(\"This time taken depends on your CPU)\")\n",
    "print(\"Progress will be saved, so you can interrupt and resume if needed\\n\")\n",
    "\n",
    "# Check if results already exist\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"⚠ WARNING: {output_file} already exists!\")\n",
    "    response = input(\"Do you want to:\\n  1) Resume from existing file\\n  2) Start fresh (delete existing)\\n  3) Cancel\\nEnter choice (1/2/3): \")\n",
    "    \n",
    "    if response == '3':\n",
    "        print(\"Cancelled by user\")\n",
    "        raise SystemExit\n",
    "    elif response == '2':\n",
    "        print(\"Starting fresh - existing file will be overwritten\")\n",
    "        ocr_results = []\n",
    "    elif response == '1':\n",
    "        print(\"Resuming from existing file...\")\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            ocr_results = json.load(f)\n",
    "        print(f\"✓ Loaded {len(ocr_results)} existing results\")\n",
    "    else:\n",
    "        print(\"Invalid choice, starting fresh\")\n",
    "        ocr_results = []\n",
    "else:\n",
    "    ocr_results = []\n",
    "\n",
    "# Get list of already processed files\n",
    "processed_files = set([result['filename'] for result in ocr_results])\n",
    "files_to_process = [f for f in image_files if f not in processed_files]\n",
    "\n",
    "print(f\"\\nTotal images: {total_images}\")\n",
    "print(f\"Already processed: {len(processed_files)}\")\n",
    "print(f\"Remaining: {len(files_to_process)}\")\n",
    "\n",
    "# Process images with progress bar\n",
    "start_time = time.time()\n",
    "errors = []\n",
    "\n",
    "for idx, img_file in enumerate(tqdm(files_to_process, desc=\"Processing images\")):\n",
    "    try:\n",
    "        # Full image path\n",
    "        img_path = os.path.join(image_path, img_file)\n",
    "        \n",
    "        # Run OCR\n",
    "        words, boxes = extract_words_and_boxes(img_path, tesseract_config)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'filename': img_file,\n",
    "            'words': words,\n",
    "            'boxes': boxes,\n",
    "            'num_words': len(words)\n",
    "        }\n",
    "        \n",
    "        ocr_results.append(result)\n",
    "        \n",
    "        # Save checkpoint every 100 images\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(ocr_results, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"\\n✓ Checkpoint saved at {len(ocr_results)} images\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing {img_file}: {str(e)}\"\n",
    "        errors.append(error_msg)\n",
    "        print(f\"\\n✗ {error_msg}\")\n",
    "        continue\n",
    "\n",
    "elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8795820a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 5] Saving final OCR results...\n",
      "✓ Saved to: ocr_results.json\n",
      "  File size: 21.54 MB\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 5: Save Final Results\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 5] Saving final OCR results...\")\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(ocr_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "print(f\"✓ Saved to: {output_file}\")\n",
    "print(f\"  File size: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "83a06d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 6] Validating OCR results...\n",
      "\n",
      "--- OCR Statistics ---\n",
      "Total images processed: 7000\n",
      "Total words extracted: 268934\n",
      "Average words per image: 38.4\n",
      "Min words per image: 0\n",
      "Max words per image: 105\n",
      "\n",
      "⚠ WARNING: 1008 images have < 10 words\n",
      "Sample filenames:\n",
      "  1002.png: 8 words\n",
      "  1011.png: 5 words\n",
      "  1021.png: 9 words\n",
      "  1030.png: 9 words\n",
      "  1055.png: 7 words\n",
      "\n",
      "✓ No errors during processing\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 6: Validate Results\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 6] Validating OCR results...\")\n",
    "\n",
    "# Basic statistics\n",
    "word_counts = [result['num_words'] for result in ocr_results]\n",
    "print(f\"\\n--- OCR Statistics ---\")\n",
    "print(f\"Total images processed: {len(ocr_results)}\")\n",
    "print(f\"Total words extracted: {sum(word_counts)}\")\n",
    "print(f\"Average words per image: {sum(word_counts) / len(word_counts):.1f}\")\n",
    "print(f\"Min words per image: {min(word_counts)}\")\n",
    "print(f\"Max words per image: {max(word_counts)}\")\n",
    "\n",
    "# Check for potential issues\n",
    "low_word_images = [r for r in ocr_results if r['num_words'] < 10]\n",
    "if low_word_images:\n",
    "    print(f\"\\n⚠ WARNING: {len(low_word_images)} images have < 10 words\")\n",
    "    print(\"Sample filenames:\")\n",
    "    for r in low_word_images[:5]:\n",
    "        print(f\"  {r['filename']}: {r['num_words']} words\")\n",
    "else:\n",
    "    print(\"\\n✓ All images have reasonable word counts\")\n",
    "\n",
    "# Show errors if any\n",
    "if errors:\n",
    "    print(f\"\\n⚠ Errors encountered: {len(errors)}\")\n",
    "    print(\"First 5 errors:\")\n",
    "    for err in errors[:5]:\n",
    "        print(f\"  {err}\")\n",
    "else:\n",
    "    print(\"\\n✓ No errors during processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a0ff1a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 7] Inspecting sample outputs...\n",
      "\n",
      "--- Sample OCR Result (Image 001.png) ---\n",
      "Filename: 001.png\n",
      "Number of words: 95\n",
      "\n",
      "First 20 words:\n",
      "   1. 18053\n",
      "   2. Jason\n",
      "   3. Forge\n",
      "   4. sult\n",
      "   5. 155\n",
      "   6. New\n",
      "   7. Alelaburgh,\n",
      "   8. OK\n",
      "   9. $6102\n",
      "  10. VAT\n",
      "  11. Number\n",
      "  12. ‘ula\n",
      "  13. Suarez\n",
      "  14. Cervantes\n",
      "  15. sc\n",
      "  16. 3048\n",
      "  17. Box\n",
      "  18. 9365\n",
      "  19. Nearagua\n",
      "  20. Invoices\n",
      "\n",
      "First 5 bounding boxes:\n",
      "  1. [54, 21, 78, 49] → word: '18053'\n",
      "  2. [82, 21, 103, 49] → word: 'Jason'\n",
      "  3. [107, 32, 130, 41] → word: 'Forge'\n",
      "  4. [133, 32, 153, 39] → word: 'sult'\n",
      "  5. [157, 32, 172, 39] → word: '155'\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 7: Sample Output Inspection\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 7] Inspecting sample outputs...\")\n",
    "\n",
    "print(\"\\n--- Sample OCR Result (Image 001.png) ---\")\n",
    "sample_result = [r for r in ocr_results if r['filename'] == '001.png'][0]\n",
    "print(f\"Filename: {sample_result['filename']}\")\n",
    "print(f\"Number of words: {sample_result['num_words']}\")\n",
    "print(f\"\\nFirst 20 words:\")\n",
    "for i, word in enumerate(sample_result['words'][:20], 1):\n",
    "    print(f\"  {i:2d}. {word}\")\n",
    "\n",
    "print(f\"\\nFirst 5 bounding boxes:\")\n",
    "for i, box in enumerate(sample_result['boxes'][:5], 1):\n",
    "    print(f\"  {i}. {box} → word: '{sample_result['words'][i-1]}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "15572b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 4 COMPLETE - SUMMARY\n",
      "================================================================================\n",
      "\n",
      "✓ Successfully processed 7000 images\n",
      "✓ Total processing time: 51.9 minutes\n",
      "✓ Average time per image: 0.44 seconds\n",
      "✓ Output file: ocr_results.json (21.54 MB)\n",
      "\n",
      "--- Output Format ---\n",
      "Each record contains:\n",
      "  - filename: Image filename\n",
      "  - words: List of extracted words\n",
      "  - boxes: List of bounding boxes [x0, y0, x1, y1]\n",
      "  - num_words: Count of words extracted\n",
      "\n",
      "--- Key Statistics ---\n",
      "Images processed: 7000\n",
      "Total words: 268,934\n",
      "Average words/image: 38.4\n",
      "\n",
      "✓ All images processed successfully\n",
      "\n",
      "================================================================================\n",
      "Ready for Phase 5: Label Alignment\n",
      "================================================================================\n",
      "\n",
      "--- Next Steps ---\n",
      "Phase 5 will:\n",
      "  1. Load cleaned_data.json (ground truth labels)\n",
      "  2. Load ocr_results.json (OCR output)\n",
      "  3. Match JSON labels to OCR words (fuzzy matching)\n",
      "  4. Assign BIO tags to each word\n",
      "  5. Create training dataset\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 8: Processing Summary\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 4 COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n✓ Successfully processed {len(ocr_results)} images\")\n",
    "print(f\"✓ Total processing time: {elapsed_time / 60:.1f} minutes\")\n",
    "print(f\"✓ Average time per image: {elapsed_time / len(ocr_results):.2f} seconds\")\n",
    "print(f\"✓ Output file: {output_file} ({file_size_mb:.2f} MB)\")\n",
    "\n",
    "print(\"\\n--- Output Format ---\")\n",
    "print(\"Each record contains:\")\n",
    "print(\"  - filename: Image filename\")\n",
    "print(\"  - words: List of extracted words\")\n",
    "print(\"  - boxes: List of bounding boxes [x0, y0, x1, y1]\")\n",
    "print(\"  - num_words: Count of words extracted\")\n",
    "\n",
    "print(\"\\n--- Key Statistics ---\")\n",
    "print(f\"Images processed: {len(ocr_results)}\")\n",
    "print(f\"Total words: {sum(word_counts):,}\")\n",
    "print(f\"Average words/image: {sum(word_counts) / len(word_counts):.1f}\")\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\n⚠ Errors: {len(errors)} images failed\")\n",
    "else:\n",
    "    print(\"\\n✓ All images processed successfully\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Ready for Phase 5: Label Alignment\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n--- Next Steps ---\")\n",
    "print(\"Phase 5 will:\")\n",
    "print(\"  1. Load cleaned_data.json (ground truth labels)\")\n",
    "print(\"  2. Load ocr_results.json (OCR output)\")\n",
    "print(\"  3. Match JSON labels to OCR words (fuzzy matching)\")\n",
    "print(\"  4. Assign BIO tags to each word\")\n",
    "print(\"  5. Create training dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "27eb6270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 5: LABEL ALIGNMENT\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from rapidfuzz import fuzz, process\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 5: LABEL ALIGNMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 5: LABEL ALIGNMENT\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c446c861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1] Defining label schema...\n",
      "✓ Defined 16 labels\n",
      "\n",
      "Label schema:\n",
      "   0: O\n",
      "   1: B-invoice_number\n",
      "   2: I-invoice_number\n",
      "   3: B-invoice_date\n",
      "   4: I-invoice_date\n",
      "   5: B-buyer_address\n",
      "   6: I-buyer_address\n",
      "   7: B-seller_address\n",
      "   8: I-seller_address\n",
      "   9: B-product_description\n",
      "  10: I-product_description\n",
      "  11: B-product_quantity\n",
      "  12: B-product_unit_price\n",
      "  13: B-product_total_price\n",
      "  14: B-payment_total\n",
      "  15: B-payment_sub_total\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 1: Define Label Schema\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 1] Defining label schema...\")\n",
    "\n",
    "# BIO tagging scheme for 7 target fields\n",
    "label2id = {\n",
    "    'O': 0,  # Outside any entity\n",
    "    \n",
    "    # Document identifiers\n",
    "    'B-invoice_number': 1,\n",
    "    'I-invoice_number': 2,\n",
    "    'B-invoice_date': 3,\n",
    "    'I-invoice_date': 4,\n",
    "    \n",
    "    # Addresses\n",
    "    'B-buyer_address': 5,\n",
    "    'I-buyer_address': 6,\n",
    "    'B-seller_address': 7,\n",
    "    'I-seller_address': 8,\n",
    "    \n",
    "    # Products\n",
    "    'B-product_description': 9,\n",
    "    'I-product_description': 10,\n",
    "    'B-product_quantity': 11,\n",
    "    'B-product_unit_price': 12,\n",
    "    'B-product_total_price': 13,\n",
    "    \n",
    "    # Payment totals\n",
    "    'B-payment_total': 14,\n",
    "    'B-payment_sub_total': 15\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "print(f\"✓ Defined {len(label2id)} labels\")\n",
    "print(\"\\nLabel schema:\")\n",
    "for label_id, label_name in sorted(id2label.items()):\n",
    "    print(f\"  {label_id:2d}: {label_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "db703448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] Loading preprocessed data...\n",
      "✓ Loaded 7000 ground truth records\n",
      "✓ Loaded 7000 OCR records\n",
      "✓ Created OCR lookup dictionary\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 2: Load Data\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 2] Loading preprocessed data...\")\n",
    "\n",
    "# Load cleaned ground truth\n",
    "with open('cleaned_data.json', 'r', encoding='utf-8') as f:\n",
    "    ground_truth_data = json.load(f)\n",
    "print(f\"✓ Loaded {len(ground_truth_data)} ground truth records\")\n",
    "\n",
    "# Load OCR results\n",
    "with open('ocr_results.json', 'r', encoding='utf-8') as f:\n",
    "    ocr_data = json.load(f)\n",
    "print(f\"✓ Loaded {len(ocr_data)} OCR records\")\n",
    "\n",
    "# Create lookup dictionary for OCR data\n",
    "ocr_lookup = {item['filename'].replace('.png', '.json'): item for item in ocr_data}\n",
    "print(f\"✓ Created OCR lookup dictionary\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "22df0c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 3] Defining matching utilities...\n",
      "✓ Matching utilities defined\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 3: String Normalization and Matching Utilities\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 3] Defining matching utilities...\")\n",
    "\n",
    "def normalize_string(s):\n",
    "    \"\"\"\n",
    "    Normalize string for matching.\n",
    "    - Lowercase\n",
    "    - Remove extra whitespace\n",
    "    - Remove punctuation (optional)\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).lower().strip()\n",
    "    # Remove extra whitespace\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return s\n",
    "\n",
    "def normalize_numeric(s):\n",
    "    \"\"\"\n",
    "    Normalize numeric strings for matching.\n",
    "    - Remove currency symbols\n",
    "    - Standardize decimal separators\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    # Remove currency symbols\n",
    "    s = re.sub(r'[$€£¥]', '', s)\n",
    "    # Remove commas in numbers\n",
    "    s = s.replace(',', '')\n",
    "    return s.strip()\n",
    "\n",
    "def fuzzy_match_word(target, word_list, threshold=70):\n",
    "    \"\"\"\n",
    "    Find best fuzzy match for target in word_list.\n",
    "    \n",
    "    Returns: (matched_word, match_score, word_index) or (None, 0, -1)\n",
    "    \"\"\"\n",
    "    if not word_list or not target:\n",
    "        return (None, 0, -1)\n",
    "    \n",
    "    # Use rapidfuzz to find best match\n",
    "    result = process.extractOne(\n",
    "        target,\n",
    "        word_list,\n",
    "        scorer=fuzz.ratio,\n",
    "        score_cutoff=threshold\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        matched_word, score, idx = result\n",
    "        return (matched_word, score, idx)\n",
    "    \n",
    "    return (None, 0, -1)\n",
    "\n",
    "def find_sequence_match(target_tokens, ocr_words, ocr_boxes, threshold=60):\n",
    "    \"\"\"\n",
    "    Find a sequence of OCR words that matches target tokens.\n",
    "    \n",
    "    Returns: List of indices in ocr_words, or []\n",
    "    \"\"\"\n",
    "    if not target_tokens or not ocr_words:\n",
    "        return []\n",
    "    \n",
    "    matched_indices = []\n",
    "    \n",
    "    for token in target_tokens:\n",
    "        # Normalize token\n",
    "        norm_token = normalize_string(token)\n",
    "        if not norm_token:\n",
    "            continue\n",
    "        \n",
    "        # Try to find fuzzy match\n",
    "        match, score, idx = fuzzy_match_word(norm_token, ocr_words, threshold=threshold)\n",
    "        \n",
    "        if match and idx not in matched_indices:\n",
    "            matched_indices.append(idx)\n",
    "    \n",
    "    # Check if matched words are roughly sequential\n",
    "    if len(matched_indices) > 1:\n",
    "        # Sort by position\n",
    "        matched_indices.sort()\n",
    "        \n",
    "        # Check if they form a reasonable sequence\n",
    "        # Allow some words to be missing but maintain general order\n",
    "        gaps = [matched_indices[i+1] - matched_indices[i] for i in range(len(matched_indices)-1)]\n",
    "\n",
    "        # If there are huge gaps (>30 words), might be wrong match\n",
    "        if any(gap > 30 for gap in gaps):\n",
    "            # Check spatial proximity instead\n",
    "            boxes = [ocr_boxes[idx] for idx in matched_indices]\n",
    "            if not are_spatially_close(boxes):\n",
    "                return []\n",
    "    \n",
    "    return matched_indices\n",
    "\n",
    "def are_spatially_close(boxes, max_distance=130):\n",
    "    \"\"\"\n",
    "    Check if bounding boxes are spatially close.\n",
    "    \"\"\"\n",
    "    if len(boxes) < 2:\n",
    "        return True\n",
    "    \n",
    "    for i in range(len(boxes) - 1):\n",
    "        box1, box2 = boxes[i], boxes[i+1]\n",
    "        \n",
    "        # Calculate distance between box centers\n",
    "        center1 = ((box1[0] + box1[2]) / 2, (box1[1] + box1[3]) / 2)\n",
    "        center2 = ((box2[0] + box2[2]) / 2, (box2[1] + box2[3]) / 2)\n",
    "        \n",
    "        distance = np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)\n",
    "        \n",
    "        if distance > max_distance:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def are_on_same_line(box1, box2, threshold=25):\n",
    "    \"\"\"\n",
    "    Check if two boxes are on the same horizontal line.\n",
    "    \"\"\"\n",
    "    y1_center = (box1[1] + box1[3]) / 2\n",
    "    y2_center = (box2[1] + box2[3]) / 2\n",
    "    \n",
    "    return abs(y1_center - y2_center) < threshold\n",
    "\n",
    "def group_by_row(words, boxes, threshold=25):\n",
    "    \"\"\"\n",
    "    Group words into rows based on Y-coordinate.\n",
    "    \n",
    "    Returns: List of rows, each containing (word, box, original_index)\n",
    "    \"\"\"\n",
    "    if not words:\n",
    "        return []\n",
    "    \n",
    "    # Create list of (word, box, index)\n",
    "    items = [(words[i], boxes[i], i) for i in range(len(words))]\n",
    "    \n",
    "    # Sort by Y-coordinate\n",
    "    items.sort(key=lambda x: (x[1][1] + x[1][3]) / 2)\n",
    "    \n",
    "    rows = []\n",
    "    current_row = [items[0]]\n",
    "    current_y = (items[0][1][1] + items[0][1][3]) / 2\n",
    "    \n",
    "    for item in items[1:]:\n",
    "        y_center = (item[1][1] + item[1][3]) / 2\n",
    "        \n",
    "        if abs(y_center - current_y) < threshold:\n",
    "            current_row.append(item)\n",
    "        else:\n",
    "            # Sort current row by X-coordinate (left to right)\n",
    "            current_row.sort(key=lambda x: x[1][0])\n",
    "            rows.append(current_row)\n",
    "            current_row = [item]\n",
    "            current_y = y_center\n",
    "    \n",
    "    if current_row:\n",
    "        current_row.sort(key=lambda x: x[1][0])\n",
    "        rows.append(current_row)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "print(\"✓ Matching utilities defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ba40ba5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 4] Defining field-specific matching functions...\n",
      "✓ Field-specific matching functions defined\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 4: Field-Specific Matching Functions\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 4] Defining field-specific matching functions...\")\n",
    "\n",
    "def match_numeric_field(field_value, ocr_words, field_name):\n",
    "    \"\"\"\n",
    "    Special ultra-lenient matching for numeric fields.\n",
    "    Handles severe OCR digit errors.\n",
    "    \n",
    "    Strategies:\n",
    "    1. Exact match (after normalization)\n",
    "    2. Partial substring match\n",
    "    3. Digit overlap match (>70% shared digits)\n",
    "    4. Fuzzy match with very low threshold (60%)\n",
    "    \n",
    "    Returns: List of (word_index, label_id)\n",
    "    \"\"\"\n",
    "    if field_value is None:\n",
    "        return []\n",
    "    \n",
    "    # Normalize target: remove $, commas, spaces\n",
    "    target = str(field_value).replace('$', '').replace(',', '').replace(' ', '').strip()\n",
    "    \n",
    "    if not target:\n",
    "        return []\n",
    "    \n",
    "    # Normalize OCR words\n",
    "    norm_words = []\n",
    "    for word in ocr_words:\n",
    "        word_clean = word.replace('$', '').replace(',', '').replace(' ', '').strip()\n",
    "        norm_words.append(word_clean)\n",
    "    \n",
    "    # Strategy 1: Exact match\n",
    "    try:\n",
    "        idx = norm_words.index(target)\n",
    "        return [(idx, label2id[f'B-{field_name}'])]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    # Strategy 2: Partial substring match\n",
    "    for idx, word_clean in enumerate(norm_words):\n",
    "        if len(word_clean) >= 3:  # Avoid single digits\n",
    "            # Check if either contains the other\n",
    "            if target in word_clean or word_clean in target:\n",
    "                # Ensure significant overlap (>60% of shorter string)\n",
    "                shorter = min(len(target), len(word_clean))\n",
    "                longer = max(len(target), len(word_clean))\n",
    "                if shorter / longer >= 0.55:\n",
    "                    return [(idx, label2id[f'B-{field_name}'])]\n",
    "    \n",
    "    # Strategy 3: Digit overlap match\n",
    "    # Useful when digits are scrambled: \"716.63\" vs \"71663\" or \"617.63\"\n",
    "    target_digits = ''.join(c for c in target if c.isdigit())\n",
    "    \n",
    "    if len(target_digits) >= 2:  # Need at least 2 digits\n",
    "        for idx, word_clean in enumerate(norm_words):\n",
    "            word_digits = ''.join(c for c in word_clean if c.isdigit())\n",
    "            \n",
    "            if len(word_digits) >= 2:\n",
    "                # Calculate digit overlap\n",
    "                common_digits = sum(1 for d in target_digits if d in word_digits)\n",
    "                total_unique = len(set(target_digits) | set(word_digits))\n",
    "                \n",
    "                if total_unique > 0:\n",
    "                    overlap_ratio = common_digits / len(target_digits)\n",
    "                    \n",
    "                    # If >70% of target digits appear in word\n",
    "                    if overlap_ratio >= 0.65:\n",
    "                        return [(idx, label2id[f'B-{field_name}'])]\n",
    "    \n",
    "    # Strategy 4: Very lenient fuzzy match (last resort)\n",
    "    for idx, word_clean in enumerate(norm_words):\n",
    "        if len(word_clean) >= 2:\n",
    "            similarity = fuzz.ratio(target, word_clean)\n",
    "            \n",
    "            if similarity >= 55:  # Very low threshold for numeric\n",
    "                return [(idx, label2id[f'B-{field_name}'])]\n",
    "    \n",
    "    return []\n",
    "\n",
    "def match_simple_field(field_value, ocr_words, field_name, threshold=70):\n",
    "    \"\"\"\n",
    "    Match a simple single-value field (invoice_number, invoice_date, etc.).\n",
    "    \n",
    "    Returns: List of (word_index, label_id)\n",
    "    \"\"\"\n",
    "    if field_value is None:\n",
    "        return []\n",
    "    \n",
    "    # For numeric fields (payment), use special numeric matching\n",
    "    if 'payment' in field_name:\n",
    "        return match_numeric_field(field_value, ocr_words, field_name)\n",
    "    \n",
    "    # For text fields, continue with regular fuzzy matching\n",
    "    # Normalize\n",
    "    norm_value = normalize_string(field_value)\n",
    "    norm_words = [normalize_string(w) for w in ocr_words]\n",
    "    \n",
    "    # Try exact match first\n",
    "    try:\n",
    "        idx = norm_words.index(norm_value)\n",
    "        label = label2id[f'B-{field_name}']\n",
    "        return [(idx, label)]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    # Try fuzzy match\n",
    "    match, score, idx = fuzzy_match_word(norm_value, norm_words, threshold=threshold)\n",
    "    \n",
    "    if match:\n",
    "        label = label2id[f'B-{field_name}']\n",
    "        return [(idx, label)]\n",
    "    \n",
    "    # Try splitting the value and matching parts\n",
    "    # This handles cases like \"05.03.2012\" split into [\"05\", \".\", \"03\", \".\", \"2012\"]\n",
    "    if '.' in str(field_value) or ' ' in str(field_value):\n",
    "        tokens = re.split(r'[\\s\\.]', str(field_value))\n",
    "        tokens = [t for t in tokens if t]  # Remove empty\n",
    "        \n",
    "        if len(tokens) > 1:\n",
    "            matches = []\n",
    "            for token in tokens:\n",
    "                norm_token = normalize_string(token)\n",
    "                match, score, idx = fuzzy_match_word(norm_token, norm_words, threshold=threshold)\n",
    "                if match:\n",
    "                    matches.append(idx)\n",
    "            \n",
    "            if matches:\n",
    "                # Check if I- tag exists for this field\n",
    "                i_tag_key = f'I-{field_name}'\n",
    "                \n",
    "                if i_tag_key in label2id:\n",
    "                    # Field has I- tag (invoice_number, invoice_date, addresses)\n",
    "                    result = [(matches[0], label2id[f'B-{field_name}'])]\n",
    "                    for idx in matches[1:]:\n",
    "                        result.append((idx, label2id[i_tag_key]))\n",
    "                    return result\n",
    "                else:\n",
    "                    # Field only has B- tag (shouldn't happen for non-payment fields)\n",
    "                    return [(matches[0], label2id[f'B-{field_name}'])]\n",
    "    \n",
    "    return []\n",
    "\n",
    "def match_address_field(address_value, ocr_words, ocr_boxes, field_name, threshold=60):\n",
    "    \"\"\"\n",
    "    Match a multi-word address field.\n",
    "    \n",
    "    Returns: List of (word_index, label_id)\n",
    "    \"\"\"\n",
    "    if address_value is None:\n",
    "        return []\n",
    "    \n",
    "    # Tokenize address\n",
    "    tokens = str(address_value).split()\n",
    "    \n",
    "    # Find sequence match\n",
    "    matched_indices = find_sequence_match(tokens, ocr_words, ocr_boxes, threshold=threshold)\n",
    "    \n",
    "    if not matched_indices:\n",
    "        return []\n",
    "    \n",
    "    # Assign BIO tags\n",
    "    result = [(matched_indices[0], label2id[f'B-{field_name}'])]\n",
    "    for idx in matched_indices[1:]:\n",
    "        result.append((idx, label2id[f'I-{field_name}']))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def match_products(products, ocr_words, ocr_boxes, threshold=60):\n",
    "    \"\"\"\n",
    "    Match product line items.\n",
    "    \n",
    "    Returns: List of (word_index, label_id)\n",
    "    \"\"\"\n",
    "    if not products:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Group OCR words into rows\n",
    "    rows = group_by_row(ocr_words, ocr_boxes)\n",
    "    \n",
    "    # For each product\n",
    "    for product in products:\n",
    "        description = product.get('description')\n",
    "        quantity = product.get('quantity')\n",
    "        unit_price = product.get('unit_price')\n",
    "        total_price = product.get('total_price')\n",
    "        \n",
    "        # Match description (multi-word)\n",
    "        if description:\n",
    "            desc_tokens = str(description).split()\n",
    "            desc_matches = find_sequence_match(desc_tokens, ocr_words, ocr_boxes, threshold=threshold)\n",
    "            \n",
    "            if desc_matches:\n",
    "                # Assign B-product_description to first, I- to rest\n",
    "                results.append((desc_matches[0], label2id['B-product_description']))\n",
    "                for idx in desc_matches[1:]:\n",
    "                    results.append((idx, label2id['I-product_description']))\n",
    "                \n",
    "                # Try to match quantity, unit_price, total_price in same row\n",
    "                # Find which row the description is in\n",
    "                desc_y = (ocr_boxes[desc_matches[0]][1] + ocr_boxes[desc_matches[0]][3]) / 2\n",
    "                \n",
    "                # Find matching row\n",
    "                matching_row = None\n",
    "                for row in rows:\n",
    "                    row_y = (row[0][1][1] + row[0][1][3]) / 2\n",
    "                    if abs(row_y - desc_y) < 20:\n",
    "                        matching_row = row\n",
    "                        break\n",
    "                \n",
    "                if matching_row:\n",
    "                    row_words = [item[0] for item in matching_row]\n",
    "                    row_indices = [item[2] for item in matching_row]\n",
    "                    \n",
    "                    # Match quantity using numeric matching\n",
    "                    if quantity is not None:\n",
    "                        qty_matches = match_numeric_field(quantity, row_words, 'product_quantity')\n",
    "                        if qty_matches:\n",
    "                            local_idx = qty_matches[0][0]\n",
    "                            global_idx = row_indices[local_idx]\n",
    "                            results.append((global_idx, label2id['B-product_quantity']))\n",
    "                    \n",
    "                    # Match unit_price using numeric matching\n",
    "                    if unit_price is not None:\n",
    "                        price_matches = match_numeric_field(unit_price, row_words, 'product_unit_price')\n",
    "                        if price_matches:\n",
    "                            local_idx = price_matches[0][0]\n",
    "                            global_idx = row_indices[local_idx]\n",
    "                            results.append((global_idx, label2id['B-product_unit_price']))\n",
    "                    \n",
    "                    # Match total_price using numeric matching\n",
    "                    if total_price is not None:\n",
    "                        total_matches = match_numeric_field(total_price, row_words, 'product_total_price')\n",
    "                        if total_matches:\n",
    "                            local_idx = total_matches[0][0]\n",
    "                            global_idx = row_indices[local_idx]\n",
    "                            results.append((global_idx, label2id['B-product_total_price']))\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Field-specific matching functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3abc1344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 5] Defining main labeling function...\n",
      "✓ Main labeling function defined\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 5: Main Labeling Function\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 5] Defining main labeling function...\")\n",
    "\n",
    "def label_invoice(ground_truth, ocr_result):\n",
    "    \"\"\"\n",
    "    Create BIO labels for a single invoice.\n",
    "    \n",
    "    Returns: (words, boxes, labels, label_strings) or None if failed\n",
    "    \"\"\"\n",
    "    ocr_words = ocr_result['words']\n",
    "    ocr_boxes = ocr_result['boxes']\n",
    "    \n",
    "    if not ocr_words:\n",
    "        return None\n",
    "    \n",
    "    # Initialize all labels as 'O' (outside)\n",
    "    labels = [0] * len(ocr_words)\n",
    "    \n",
    "    # Track which indices have been labeled\n",
    "    labeled_indices = set()\n",
    "    \n",
    "    # Match simple fields\n",
    "    simple_fields = [\n",
    "        ('invoice_number', ground_truth.get('invoice_number')),\n",
    "        ('invoice_date', ground_truth.get('invoice_date')),\n",
    "        ('payment_total', ground_truth.get('payment_total')),\n",
    "        ('payment_sub_total', ground_truth.get('payment_sub_total'))\n",
    "    ]\n",
    "    \n",
    "    for field_name, field_value in simple_fields:\n",
    "        matches = match_simple_field(field_value, ocr_words, field_name)\n",
    "        for idx, label_id in matches:\n",
    "            if idx not in labeled_indices:\n",
    "                labels[idx] = label_id\n",
    "                labeled_indices.add(idx)\n",
    "    \n",
    "    # Match address fields\n",
    "    address_fields = [\n",
    "        ('buyer_address', ground_truth.get('buyer_address')),\n",
    "        ('seller_address', ground_truth.get('seller_address'))\n",
    "    ]\n",
    "    \n",
    "    for field_name, field_value in address_fields:\n",
    "        matches = match_address_field(field_value, ocr_words, ocr_boxes, field_name)\n",
    "        for idx, label_id in matches:\n",
    "            if idx not in labeled_indices:\n",
    "                labels[idx] = label_id\n",
    "                labeled_indices.add(idx)\n",
    "    \n",
    "    # Match products\n",
    "    products = ground_truth.get('products', [])\n",
    "    product_matches = match_products(products, ocr_words, ocr_boxes)\n",
    "    for idx, label_id in product_matches:\n",
    "        if idx not in labeled_indices:\n",
    "            labels[idx] = label_id\n",
    "            labeled_indices.add(idx)\n",
    "    \n",
    "    # Convert numeric labels to strings for readability\n",
    "    label_strings = [id2label[label_id] for label_id in labels]\n",
    "    \n",
    "    return (ocr_words, ocr_boxes, labels, label_strings)\n",
    "\n",
    "print(\"✓ Main labeling function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "633bdb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 6] Processing all invoices...\n",
      "\n",
      "Labeling 7000 invoices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling invoices: 100%|██████████| 7000/7000 [00:05<00:00, 1346.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully labeled 6937 invoices\n",
      "⚠ Failed to label 63 invoices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 6: Process All Invoices\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 6] Processing all invoices...\")\n",
    "\n",
    "labeled_dataset = []\n",
    "failed_count = 0\n",
    "label_stats = defaultdict(int)\n",
    "\n",
    "print(f\"\\nLabeling {len(ground_truth_data)} invoices...\")\n",
    "\n",
    "for gt_record in tqdm(ground_truth_data, desc=\"Labeling invoices\"):\n",
    "    filename = gt_record['filename']\n",
    "    \n",
    "    # Get corresponding OCR result\n",
    "    ocr_result = ocr_lookup.get(filename)\n",
    "    \n",
    "    if not ocr_result:\n",
    "        failed_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Label the invoice\n",
    "    result = label_invoice(gt_record, ocr_result)\n",
    "    \n",
    "    if result is None:\n",
    "        failed_count += 1\n",
    "        continue\n",
    "    \n",
    "    words, boxes, labels, label_strings = result\n",
    "    \n",
    "    # Store labeled sample\n",
    "    labeled_sample = {\n",
    "        'filename': filename,\n",
    "        'words': words,\n",
    "        'boxes': boxes,\n",
    "        'labels': labels,\n",
    "        'label_strings': label_strings\n",
    "    }\n",
    "    \n",
    "    labeled_dataset.append(labeled_sample)\n",
    "    \n",
    "    # Update label statistics\n",
    "    for label in labels:\n",
    "        label_stats[label] += 1\n",
    "\n",
    "print(f\"\\n✓ Successfully labeled {len(labeled_dataset)} invoices\")\n",
    "if failed_count > 0:\n",
    "    print(f\"⚠ Failed to label {failed_count} invoices\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1906212a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 7] Saving labeled dataset...\n",
      "✓ Saved to: labeled_dataset.json\n",
      "  File size: 27.68 MB\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 7: Save Labeled Dataset\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 7] Saving labeled dataset...\")\n",
    "\n",
    "output_file = \"labeled_dataset.json\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(labeled_dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "print(f\"✓ Saved to: {output_file}\")\n",
    "print(f\"  File size: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fec49a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 8] Analyzing label distribution...\n",
      "\n",
      "--- Label Distribution ---\n",
      "Total tokens: 268,934\n",
      "\n",
      "Label                          Count      Percentage\n",
      "--------------------------------------------------\n",
      "O                              218,649     81.30%\n",
      "B-invoice_number               1,667        0.62%\n",
      "B-invoice_date                 2,523        0.94%\n",
      "I-invoice_date                 68           0.03%\n",
      "B-buyer_address                3,717        1.38%\n",
      "I-buyer_address                7,282        2.71%\n",
      "B-seller_address               3,024        1.12%\n",
      "I-seller_address               7,594        2.82%\n",
      "B-product_description          7,632        2.84%\n",
      "I-product_description          6,977        2.59%\n",
      "B-product_quantity             2,710        1.01%\n",
      "B-product_unit_price           1,496        0.56%\n",
      "B-product_total_price          1,517        0.56%\n",
      "B-payment_total                2,077        0.77%\n",
      "B-payment_sub_total            2,001        0.74%\n",
      "\n",
      "--- Coverage Statistics ---\n",
      "'O' (Outside) tokens: 218,649 (81.3%)\n",
      "Entity tokens: 50,285 (18.7%)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 8: Analyze Label Distribution\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 8] Analyzing label distribution...\")\n",
    "\n",
    "total_tokens = sum(label_stats.values())\n",
    "\n",
    "print(f\"\\n--- Label Distribution ---\")\n",
    "print(f\"Total tokens: {total_tokens:,}\")\n",
    "print(f\"\\n{'Label':<30} {'Count':<10} {'Percentage':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for label_id in sorted(label_stats.keys()):\n",
    "    label_name = id2label[label_id]\n",
    "    count = label_stats[label_id]\n",
    "    percentage = (count / total_tokens) * 100\n",
    "    print(f\"{label_name:<30} {count:<10,} {percentage:>6.2f}%\")\n",
    "\n",
    "# Calculate entity coverage\n",
    "entity_tokens = total_tokens - label_stats[0]  # Exclude 'O'\n",
    "entity_percentage = (entity_tokens / total_tokens) * 100\n",
    "\n",
    "print(f\"\\n--- Coverage Statistics ---\")\n",
    "print(f\"'O' (Outside) tokens: {label_stats[0]:,} ({label_stats[0]/total_tokens*100:.1f}%)\")\n",
    "print(f\"Entity tokens: {entity_tokens:,} ({entity_percentage:.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "65034e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 9] Inspecting sample outputs...\n",
      "\n",
      "--- Sample 1: Invoice 001.png ---\n",
      "Filename: 001.json\n",
      "Number of words: 95\n",
      "\n",
      "First 30 words with labels:\n",
      "   1. 18053                → O\n",
      "   2. Jason                → O\n",
      "   3. Forge                → O\n",
      "   4. sult                 → O\n",
      "   5. 155                  → O\n",
      "   6. New                  → O\n",
      "   7. Alelaburgh,          → O\n",
      "   8. OK                   → O\n",
      "   9. $6102                → O\n",
      "  10. VAT                  → O\n",
      "  11. Number               → O\n",
      "  12. ‘ula                 → O\n",
      "  13. Suarez               → O\n",
      "  14. Cervantes            → O\n",
      "  15. sc                   → B-buyer_address\n",
      "  16. 3048                 → I-buyer_address\n",
      "  17. Box                  → I-buyer_address\n",
      "  18. 9365                 → I-buyer_address\n",
      "  19. Nearagua             → O\n",
      "  20. Invoices             → O\n",
      "  21. Date                 → O\n",
      "  22. 08032012             → B-invoice_date\n",
      "  23. ‘Amount              → O\n",
      "  24. Bue                  → O\n",
      "  25. MRO                  → O\n",
      "  26. 138.01               → O\n",
      "  27. tem                  → O\n",
      "  28. |Description         → O\n",
      "  29. Unit                 → O\n",
      "  30. Cost                 → O\n",
      "\n",
      "Label distribution for this sample:\n",
      "  O                               73\n",
      "  I-product_description            6\n",
      "  I-buyer_address                  4\n",
      "  B-product_description            3\n",
      "  B-product_quantity               2\n",
      "  B-product_total_price            2\n",
      "  B-buyer_address                  1\n",
      "  B-invoice_date                   1\n",
      "  B-product_unit_price             1\n",
      "  B-payment_total                  1\n",
      "  B-payment_sub_total              1\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 9: Sample Inspection\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 9] Inspecting sample outputs...\")\n",
    "\n",
    "print(\"\\n--- Sample 1: Invoice 001.png ---\")\n",
    "sample1 = [s for s in labeled_dataset if s['filename'] == '001.json'][0]\n",
    "\n",
    "print(f\"Filename: {sample1['filename']}\")\n",
    "print(f\"Number of words: {len(sample1['words'])}\")\n",
    "print(f\"\\nFirst 30 words with labels:\")\n",
    "for i in range(min(30, len(sample1['words']))):\n",
    "    word = sample1['words'][i]\n",
    "    label = sample1['label_strings'][i]\n",
    "    print(f\"  {i+1:2d}. {word:<20} → {label}\")\n",
    "\n",
    "# Show label distribution for this sample\n",
    "sample_label_counts = defaultdict(int)\n",
    "for label in sample1['label_strings']:\n",
    "    sample_label_counts[label] += 1\n",
    "\n",
    "print(f\"\\nLabel distribution for this sample:\")\n",
    "for label, count in sorted(sample_label_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {label:<30} {count:>3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8b9700d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 10] Validating label quality...\n",
      "\n",
      "⚠ Found 2246 samples with <10% entity labels:\n",
      "  035.json: 2/22 entities (9.1%)\n",
      "  040.json: 1/26 entities (3.8%)\n",
      "  077.json: 2/37 entities (5.4%)\n",
      "  079.json: 2/28 entities (7.1%)\n",
      "  096.json: 2/22 entities (9.1%)\n",
      "  1002.json: 0/8 entities (0.0%)\n",
      "  1004.json: 1/15 entities (6.7%)\n",
      "  1008.json: 2/32 entities (6.2%)\n",
      "  1011.json: 0/5 entities (0.0%)\n",
      "  1016.json: 0/19 entities (0.0%)\n",
      "\n",
      "--- Label Coverage Across Samples ---\n",
      "Label                          Samples    Coverage  \n",
      "--------------------------------------------------\n",
      "B-invoice_number               1,667        24.0%\n",
      "B-invoice_date                 2,523        36.4%\n",
      "I-invoice_date                 65            0.9%\n",
      "B-buyer_address                3,717        53.6%\n",
      "I-buyer_address                2,422        34.9%\n",
      "B-seller_address               3,024        43.6%\n",
      "I-seller_address               2,343        33.8%\n",
      "B-product_description          4,012        57.8%\n",
      "I-product_description          2,144        30.9%\n",
      "B-product_quantity             1,656        23.9%\n",
      "B-product_unit_price           1,120        16.1%\n",
      "B-product_total_price          1,107        16.0%\n",
      "B-payment_total                2,077        29.9%\n",
      "B-payment_sub_total            2,001        28.8%\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 10: Quality Validation\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 10] Validating label quality...\")\n",
    "\n",
    "# Check for samples with very few entity labels\n",
    "low_entity_samples = []\n",
    "for sample in labeled_dataset:\n",
    "    entity_count = sum(1 for label in sample['labels'] if label != 0)\n",
    "    total_count = len(sample['labels'])\n",
    "    entity_ratio = entity_count / total_count if total_count > 0 else 0\n",
    "    \n",
    "    if entity_ratio < 0.1:  # Less than 10% entities\n",
    "        low_entity_samples.append((sample['filename'], entity_ratio, entity_count, total_count))\n",
    "\n",
    "if low_entity_samples:\n",
    "    print(f\"\\n⚠ Found {len(low_entity_samples)} samples with <10% entity labels:\")\n",
    "    for filename, ratio, entity_count, total_count in low_entity_samples[:10]:\n",
    "        print(f\"  {filename}: {entity_count}/{total_count} entities ({ratio*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\n✓ All samples have reasonable entity label density\")\n",
    "\n",
    "# Check label distribution across samples\n",
    "label_coverage = defaultdict(int)\n",
    "for sample in labeled_dataset:\n",
    "    present_labels = set(sample['labels'])\n",
    "    for label_id in present_labels:\n",
    "        if label_id != 0:  # Exclude 'O'\n",
    "            label_coverage[label_id] += 1\n",
    "\n",
    "print(f\"\\n--- Label Coverage Across Samples ---\")\n",
    "print(f\"{'Label':<30} {'Samples':<10} {'Coverage':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for label_id in sorted(label_coverage.keys()):\n",
    "    label_name = id2label[label_id]\n",
    "    count = label_coverage[label_id]\n",
    "    coverage = (count / len(labeled_dataset)) * 100\n",
    "    print(f\"{label_name:<30} {count:<10,} {coverage:>6.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "df984d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 5 COMPLETE - SUMMARY\n",
      "================================================================================\n",
      "\n",
      "✓ Successfully processed 6937 invoices\n",
      "✓ Total tokens labeled: 268,934\n",
      "✓ Entity tokens: 50,285 (18.7%)\n",
      "✓ Output file: labeled_dataset.json (27.68 MB)\n",
      "\n",
      "--- Label Schema Used ---\n",
      "Total labels: 16\n",
      "  0: O\n",
      "  1: B-invoice_number\n",
      "  2: I-invoice_number\n",
      "  3: B-invoice_date\n",
      "  4: I-invoice_date\n",
      "  ...\n",
      "\n",
      "--- Key Metrics ---\n",
      "Average words per invoice: 38.8\n",
      "Average entity tokens per invoice: 7.2\n",
      "\n",
      "⚠ Failed invoices: 63\n",
      "  Success rate: 99.1%\n",
      "\n",
      "================================================================================\n",
      "Ready for Phase 6: Dataset Creation\n",
      "================================================================================\n",
      "\n",
      "--- Next Steps ---\n",
      "Phase 6 will:\n",
      "  1. Split labeled_dataset.json into train/val/test\n",
      "  2. Apply LayoutLM processor (tokenization, bbox normalization)\n",
      "  3. Create HuggingFace dataset format\n",
      "  4. Prepare for training on Kaggle\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 11: Summary Report\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 5 COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n✓ Successfully processed {len(labeled_dataset)} invoices\")\n",
    "print(f\"✓ Total tokens labeled: {total_tokens:,}\")\n",
    "print(f\"✓ Entity tokens: {entity_tokens:,} ({entity_percentage:.1f}%)\")\n",
    "print(f\"✓ Output file: {output_file} ({file_size_mb:.2f} MB)\")\n",
    "\n",
    "print(\"\\n--- Label Schema Used ---\")\n",
    "print(f\"Total labels: {len(label2id)}\")\n",
    "for label_id, label_name in sorted(id2label.items())[:5]:\n",
    "    print(f\"  {label_id}: {label_name}\")\n",
    "print(\"  ...\")\n",
    "\n",
    "print(\"\\n--- Key Metrics ---\")\n",
    "print(f\"Average words per invoice: {total_tokens / len(labeled_dataset):.1f}\")\n",
    "print(f\"Average entity tokens per invoice: {entity_tokens / len(labeled_dataset):.1f}\")\n",
    "\n",
    "if failed_count > 0:\n",
    "    print(f\"\\n⚠ Failed invoices: {failed_count}\")\n",
    "    print(f\"  Success rate: {len(labeled_dataset)/(len(labeled_dataset)+failed_count)*100:.1f}%\")\n",
    "else:\n",
    "    print(\"\\n✓ 100% success rate - all invoices labeled\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Ready for Phase 6: Dataset Creation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n--- Next Steps ---\")\n",
    "print(\"Phase 6 will:\")\n",
    "print(\"  1. Split labeled_dataset.json into train/val/test\")\n",
    "print(\"  2. Apply LayoutLM processor (tokenization, bbox normalization)\")\n",
    "print(\"  3. Create HuggingFace dataset format\")\n",
    "print(\"  4. Prepare for training on Kaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1271f292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data1/studying/projects and tasks/invoiceOCR/ocr-venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 6: DATASET CREATION (MEMORY-SAFE VERSION)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import LayoutLMv3Processor\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import gc  # Garbage collection\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 6: DATASET CREATION (MEMORY-SAFE VERSION)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 6: DATASET CREATION (MEMORY-SAFE VERSION)\")\n",
    "print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9533e39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1] Loading labeled dataset...\n",
      "✓ Loaded 6937 labeled samples\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 1: Load Labeled Data\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 1] Loading labeled dataset...\")\n",
    "\n",
    "with open('labeled_dataset.json', 'r', encoding='utf-8') as f:\n",
    "    labeled_data = json.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(labeled_data)} labeled samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa48862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] Splitting into train/val/test sets...\n",
      "✓ Train set: 5549 samples (80.0%)\n",
      "✓ Val set: 694 samples (10.0%)\n",
      "✓ Test set: 694 samples (10.0%)\n",
      "✓ No data leakage detected\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 2: Train/Val/Test Split\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 2] Splitting into train/val/test sets...\")\n",
    "\n",
    "# First split: 80% train, 20% temp\n",
    "train_data, temp_data = train_test_split(\n",
    "    labeled_data,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Second split: Split temp into 50/50 = 10% val, 10% test\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Train set: {len(train_data)} samples ({len(train_data)/len(labeled_data)*100:.1f}%)\")\n",
    "print(f\"✓ Val set: {len(val_data)} samples ({len(val_data)/len(labeled_data)*100:.1f}%)\")\n",
    "print(f\"✓ Test set: {len(test_data)} samples ({len(test_data)/len(labeled_data)*100:.1f}%)\")\n",
    "\n",
    "# Verify no overlap\n",
    "train_files = set([s['filename'] for s in train_data])\n",
    "val_files = set([s['filename'] for s in val_data])\n",
    "test_files = set([s['filename'] for s in test_data])\n",
    "\n",
    "assert len(train_files & val_files) == 0, \"Train/Val overlap!\"\n",
    "assert len(train_files & test_files) == 0, \"Train/Test overlap!\"\n",
    "assert len(val_files & test_files) == 0, \"Val/Test overlap!\"\n",
    "print(\"✓ No data leakage detected\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae1d0e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 3] Initializing LayoutLMv3 processor...\n",
      "✓ Processor initialized\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 3: Initialize LayoutLMv3 Processor\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 3] Initializing LayoutLMv3 processor...\")\n",
    "\n",
    "processor = LayoutLMv3Processor.from_pretrained(\n",
    "    \"microsoft/layoutlmv3-base\",\n",
    "    apply_ocr=False\n",
    ")\n",
    "\n",
    "print(\"✓ Processor initialized\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0927e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 4] Defining processing functions...\n",
      "✓ Functions defined\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 4: Define Functions\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 4] Defining processing functions...\")\n",
    "\n",
    "image_dir = \"./7000_invoice_image_and_json_1/images\"\n",
    "\n",
    "def load_image(filename):\n",
    "    \"\"\"Load image from filename.\"\"\"\n",
    "    img_filename = filename.replace('.json', '.png')\n",
    "    img_path = os.path.join(image_dir, img_filename)\n",
    "    image = Image.open(img_path)\n",
    "    if image.mode == 'RGBA':\n",
    "        image = image.convert('RGB')\n",
    "    return image\n",
    "\n",
    "def process_sample(sample):\n",
    "    \"\"\"Process single sample.\"\"\"\n",
    "    try:\n",
    "        image = load_image(sample['filename'])\n",
    "        words = sample['words']\n",
    "        boxes = sample['boxes']\n",
    "        labels = sample['labels']\n",
    "        \n",
    "        encoding = processor(\n",
    "            image,\n",
    "            words,\n",
    "            boxes=boxes,\n",
    "            word_labels=labels,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        processed = {\n",
    "            'pixel_values': encoding['pixel_values'].squeeze(0).numpy(),\n",
    "            'input_ids': encoding['input_ids'].squeeze(0).tolist(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0).tolist(),\n",
    "            'bbox': encoding['bbox'].squeeze(0).tolist(),\n",
    "            'labels': encoding['labels'].squeeze(0).tolist()\n",
    "        }\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {sample['filename']}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"✓ Functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73a92bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 5] Defining memory-safe batch processing...\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 5: Memory-Safe Processing Function\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 5] Defining memory-safe batch processing...\")\n",
    "\n",
    "def process_and_save_split(split_data, split_name, output_path, chunk_size=300):\n",
    "    \"\"\"\n",
    "    Process split in small chunks to avoid memory overflow.\n",
    "    Saves directly to disk, never holds full dataset in RAM.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {split_name.upper()} SET\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total samples: {len(split_data)}\")\n",
    "    print(f\"Chunk size: {chunk_size}\")\n",
    "    \n",
    "    num_chunks = (len(split_data) + chunk_size - 1) // chunk_size\n",
    "    print(f\"Number of chunks: {num_chunks}\")\n",
    "    \n",
    "    chunk_datasets = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    for chunk_idx in range(num_chunks):\n",
    "        print(f\"\\n--- Chunk {chunk_idx + 1}/{num_chunks} ---\")\n",
    "        \n",
    "        # Get chunk slice\n",
    "        start_idx = chunk_idx * chunk_size\n",
    "        end_idx = min((chunk_idx + 1) * chunk_size, len(split_data))\n",
    "        chunk_data = split_data[start_idx:end_idx]\n",
    "        \n",
    "        print(f\"Processing samples {start_idx} to {end_idx-1} ({len(chunk_data)} samples)...\")\n",
    "        \n",
    "        # Process chunk\n",
    "        processed_samples = []\n",
    "        for sample in tqdm(chunk_data, desc=f\"Processing\"):\n",
    "            processed = process_sample(sample)\n",
    "            if processed is not None:\n",
    "                processed_samples.append(processed)\n",
    "            else:\n",
    "                failed_count += 1\n",
    "        \n",
    "        # Convert chunk to dataset\n",
    "        if processed_samples:\n",
    "            print(f\"Converting to dataset...\")\n",
    "            chunk_dict = {\n",
    "                'pixel_values': [s['pixel_values'] for s in processed_samples],\n",
    "                'input_ids': [s['input_ids'] for s in processed_samples],\n",
    "                'attention_mask': [s['attention_mask'] for s in processed_samples],\n",
    "                'bbox': [s['bbox'] for s in processed_samples],\n",
    "                'labels': [s['labels'] for s in processed_samples]\n",
    "            }\n",
    "            \n",
    "            chunk_dataset = Dataset.from_dict(chunk_dict)\n",
    "            chunk_datasets.append(chunk_dataset)\n",
    "            \n",
    "            print(f\"✓ Chunk {chunk_idx + 1} complete: {len(chunk_dataset)} samples\")\n",
    "            \n",
    "            # CRITICAL: Clear memory immediately\n",
    "            del processed_samples\n",
    "            del chunk_dict\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"⚠ Chunk {chunk_idx + 1} produced no valid samples\")\n",
    "    \n",
    "    # Concatenate all chunks\n",
    "    print(f\"\\nConcatenating {len(chunk_datasets)} chunks...\")\n",
    "    final_dataset = concatenate_datasets(chunk_datasets)\n",
    "    \n",
    "    # Save to disk\n",
    "    print(f\"Saving to disk: {output_path}\")\n",
    "    final_dataset.save_to_disk(output_path)\n",
    "    \n",
    "    successful = len(split_data) - failed_count\n",
    "    print(f\"\\n✓ {split_name.upper()} COMPLETE\")\n",
    "    print(f\"  Successful: {successful}/{len(split_data)} samples\")\n",
    "    if failed_count > 0:\n",
    "        print(f\"  Failed: {failed_count} samples\")\n",
    "    \n",
    "    # Clear memory before returning\n",
    "    del chunk_datasets\n",
    "    del final_dataset\n",
    "    gc.collect()\n",
    "    \n",
    "    return successful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e653a75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 6] Processing all splits (memory-safe)...\n",
      "\n",
      "🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 🔵 \n",
      "\n",
      "============================================================\n",
      "Processing TRAIN SET\n",
      "============================================================\n",
      "Total samples: 5549\n",
      "Chunk size: 300\n",
      "Number of chunks: 19\n",
      "\n",
      "--- Chunk 1/19 ---\n",
      "Processing samples 0 to 299 (300 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:10<00:00, 28.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 1 complete: 300 samples\n",
      "\n",
      "--- Chunk 2/19 ---\n",
      "Processing samples 300 to 599 (300 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:10<00:00, 27.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 2 complete: 300 samples\n",
      "\n",
      "--- Chunk 3/19 ---\n",
      "Processing samples 600 to 899 (300 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:11<00:00, 26.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 3 complete: 300 samples\n",
      "\n",
      "--- Chunk 4/19 ---\n",
      "Processing samples 900 to 1199 (300 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:11<00:00, 26.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 4 complete: 300 samples\n",
      "\n",
      "--- Chunk 5/19 ---\n",
      "Processing samples 1200 to 1499 (300 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:10<00:00, 27.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 5 complete: 300 samples\n",
      "\n",
      "--- Chunk 6/19 ---\n",
      "Processing samples 1500 to 1799 (300 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:10<00:00, 28.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 6 complete: 300 samples\n",
      "\n",
      "--- Chunk 7/19 ---\n",
      "Processing samples 1800 to 2099 (300 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:11<00:00, 27.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 7 complete: 300 samples\n",
      "\n",
      "--- Chunk 8/19 ---\n",
      "Processing samples 2100 to 2399 (300 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:10<00:00, 29.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 8 complete: 300 samples\n",
      "\n",
      "--- Chunk 9/19 ---\n",
      "Processing samples 2400 to 2699 (300 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:10<00:00, 29.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 9 complete: 300 samples\n",
      "\n",
      "--- Chunk 10/19 ---\n",
      "Processing samples 2700 to 2999 (300 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:09<00:00, 31.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 10 complete: 300 samples\n",
      "\n",
      "--- Chunk 11/19 ---\n",
      "Processing samples 3000 to 3299 (300 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:09<00:00, 30.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 11 complete: 300 samples\n",
      "\n",
      "--- Chunk 12/19 ---\n",
      "Processing samples 3300 to 3599 (300 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:09<00:00, 30.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 12 complete: 300 samples\n",
      "\n",
      "--- Chunk 13/19 ---\n",
      "Processing samples 3600 to 3899 (300 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:10<00:00, 29.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 13 complete: 300 samples\n",
      "\n",
      "--- Chunk 14/19 ---\n",
      "Processing samples 3900 to 4199 (300 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:10<00:00, 28.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 14 complete: 300 samples\n",
      "\n",
      "--- Chunk 15/19 ---\n",
      "Processing samples 4200 to 4499 (300 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:11<00:00, 26.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 15 complete: 300 samples\n",
      "\n",
      "--- Chunk 16/19 ---\n",
      "Processing samples 4500 to 4799 (300 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:12<00:00, 24.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 16 complete: 300 samples\n",
      "\n",
      "--- Chunk 17/19 ---\n",
      "Processing samples 4800 to 5099 (300 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:10<00:00, 27.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 17 complete: 300 samples\n",
      "\n",
      "--- Chunk 18/19 ---\n",
      "Processing samples 5100 to 5399 (300 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:10<00:00, 27.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 18 complete: 300 samples\n",
      "\n",
      "--- Chunk 19/19 ---\n",
      "Processing samples 5400 to 5548 (149 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 149/149 [00:05<00:00, 25.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 19 complete: 149 samples\n",
      "\n",
      "Concatenating 19 chunks...\n",
      "Saving to disk: ./layoutlm_dataset/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (7/7 shards): 100%|██████████| 5549/5549 [00:41<00:00, 132.67 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ TRAIN COMPLETE\n",
      "  Successful: 5549/5549 samples\n",
      "\n",
      "🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 🟢 \n",
      "\n",
      "============================================================\n",
      "Processing VAL SET\n",
      "============================================================\n",
      "Total samples: 694\n",
      "Chunk size: 200\n",
      "Number of chunks: 4\n",
      "\n",
      "--- Chunk 1/4 ---\n",
      "Processing samples 0 to 199 (200 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 200/200 [00:10<00:00, 19.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 1 complete: 200 samples\n",
      "\n",
      "--- Chunk 2/4 ---\n",
      "Processing samples 200 to 399 (200 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 200/200 [00:08<00:00, 24.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 2 complete: 200 samples\n",
      "\n",
      "--- Chunk 3/4 ---\n",
      "Processing samples 400 to 599 (200 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 200/200 [00:08<00:00, 23.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 3 complete: 200 samples\n",
      "\n",
      "--- Chunk 4/4 ---\n",
      "Processing samples 600 to 693 (94 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 94/94 [00:03<00:00, 27.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 4 complete: 94 samples\n",
      "\n",
      "Concatenating 4 chunks...\n",
      "Saving to disk: ./layoutlm_dataset/val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 694/694 [00:06<00:00, 105.43 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ VAL COMPLETE\n",
      "  Successful: 694/694 samples\n",
      "\n",
      "🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 🟡 \n",
      "\n",
      "============================================================\n",
      "Processing TEST SET\n",
      "============================================================\n",
      "Total samples: 694\n",
      "Chunk size: 200\n",
      "Number of chunks: 4\n",
      "\n",
      "--- Chunk 1/4 ---\n",
      "Processing samples 0 to 199 (200 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 200/200 [00:09<00:00, 21.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 1 complete: 200 samples\n",
      "\n",
      "--- Chunk 2/4 ---\n",
      "Processing samples 200 to 399 (200 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 200/200 [00:07<00:00, 26.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 2 complete: 200 samples\n",
      "\n",
      "--- Chunk 3/4 ---\n",
      "Processing samples 400 to 599 (200 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 200/200 [00:10<00:00, 18.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 3 complete: 200 samples\n",
      "\n",
      "--- Chunk 4/4 ---\n",
      "Processing samples 600 to 693 (94 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 94/94 [00:04<00:00, 23.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dataset...\n",
      "✓ Chunk 4 complete: 94 samples\n",
      "\n",
      "Concatenating 4 chunks...\n",
      "Saving to disk: ./layoutlm_dataset/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 694/694 [00:04<00:00, 142.64 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ TEST COMPLETE\n",
      "  Successful: 694/694 samples\n",
      "\n",
      "================================================================================\n",
      "ALL SPLITS PROCESSED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 6: Process All Splits with Memory Safety\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 6] Processing all splits (memory-safe)...\")\n",
    "\n",
    "output_dir = \"./layoutlm_dataset\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process train (largest - use smaller chunks)\n",
    "print(\"\\n\" + \"🔵 \" * 30)\n",
    "train_count = process_and_save_split(\n",
    "    train_data,\n",
    "    \"train\",\n",
    "    os.path.join(output_dir, \"train\"),\n",
    "    chunk_size=300  # Small chunks for safety\n",
    ")\n",
    "\n",
    "# Process val\n",
    "print(\"\\n\" + \"🟢 \" * 30)\n",
    "val_count = process_and_save_split(\n",
    "    val_data,\n",
    "    \"val\",\n",
    "    os.path.join(output_dir, \"val\"),\n",
    "    chunk_size=200\n",
    ")\n",
    "\n",
    "# Process test\n",
    "print(\"\\n\" + \"🟡 \" * 30)\n",
    "test_count = process_and_save_split(\n",
    "    test_data,\n",
    "    \"test\",\n",
    "    os.path.join(output_dir, \"test\"),\n",
    "    chunk_size=200\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL SPLITS PROCESSED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea1fc6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 7] Calculating dataset sizes...\n",
      "\n",
      "--- Dataset Sizes ---\n",
      "Train: 3333.51 MB (5549 samples)\n",
      "Val: 416.92 MB (694 samples)\n",
      "Test: 416.92 MB (694 samples)\n",
      "Total: 4167.34 MB\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 7: Calculate Sizes\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 7] Calculating dataset sizes...\")\n",
    "\n",
    "def get_dir_size(path):\n",
    "    \"\"\"Get directory size in MB.\"\"\"\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total += os.path.getsize(filepath)\n",
    "    return total / (1024**2)\n",
    "\n",
    "train_size = get_dir_size(os.path.join(output_dir, \"train\"))\n",
    "val_size = get_dir_size(os.path.join(output_dir, \"val\"))\n",
    "test_size = get_dir_size(os.path.join(output_dir, \"test\"))\n",
    "total_size = train_size + val_size + test_size\n",
    "\n",
    "print(f\"\\n--- Dataset Sizes ---\")\n",
    "print(f\"Train: {train_size:.2f} MB ({train_count} samples)\")\n",
    "print(f\"Val: {val_size:.2f} MB ({val_count} samples)\")\n",
    "print(f\"Test: {test_size:.2f} MB ({test_count} samples)\")\n",
    "print(f\"Total: {total_size:.2f} MB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c72110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 8] Loading datasets for validation...\n",
      "✓ Train loaded: 5549 samples\n",
      "✓ Val loaded: 694 samples\n",
      "✓ Test loaded: 694 samples\n",
      "\n",
      "--- Sample Validation ---\n",
      "Keys: ['pixel_values', 'input_ids', 'attention_mask', 'bbox', 'labels']\n",
      "pixel_values shape: (3, 224, 224)\n",
      "input_ids length: 512\n",
      "bbox length: 512\n",
      "labels length: 512\n",
      "✓ All validations passed!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 8: Load and Validate\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 8] Loading datasets for validation...\")\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "train_dataset = load_from_disk(os.path.join(output_dir, \"train\"))\n",
    "val_dataset = load_from_disk(os.path.join(output_dir, \"val\"))\n",
    "test_dataset = load_from_disk(os.path.join(output_dir, \"test\"))\n",
    "\n",
    "print(f\"✓ Train loaded: {len(train_dataset)} samples\")\n",
    "print(f\"✓ Val loaded: {len(val_dataset)} samples\")\n",
    "print(f\"✓ Test loaded: {len(test_dataset)} samples\")\n",
    "\n",
    "# Validate one sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\n--- Sample Validation ---\")\n",
    "print(f\"Keys: {list(sample.keys())}\")\n",
    "print(f\"pixel_values shape: {np.array(sample['pixel_values']).shape}\")\n",
    "print(f\"input_ids length: {len(sample['input_ids'])}\")\n",
    "print(f\"bbox length: {len(sample['bbox'])}\")\n",
    "print(f\"labels length: {len(sample['labels'])}\")\n",
    "\n",
    "assert np.array(sample['pixel_values']).shape == (3, 224, 224)\n",
    "assert len(sample['input_ids']) == 512\n",
    "assert len(sample['bbox']) == 512\n",
    "assert len(sample['labels']) == 512\n",
    "print(\"✓ All validations passed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1db5b165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 9] Analyzing label distributions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Train: 100%|██████████| 5549/5549 [08:37<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 40,457/220,003 entity tokens (18.4%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Val: 100%|██████████| 694/694 [01:02<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: 5,119/27,124 entity tokens (18.9%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Test: 100%|██████████| 694/694 [01:02<00:00, 11.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 5,254/27,913 entity tokens (18.8%)\n",
      "\n",
      "✓ Similar distributions across splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# STEP 9: Label Distribution Analysis\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[STEP 9] Analyzing label distributions...\")\n",
    "\n",
    "def analyze_labels(dataset, name):\n",
    "    \"\"\"Analyze label distribution.\"\"\"\n",
    "    total_tokens = 0\n",
    "    entity_tokens = 0\n",
    "    \n",
    "    for sample in tqdm(dataset, desc=f\"Analyzing {name}\"):\n",
    "        for label in sample['labels']:\n",
    "            if label != -100:\n",
    "                total_tokens += 1\n",
    "                if label != 0:\n",
    "                    entity_tokens += 1\n",
    "    \n",
    "    entity_pct = (entity_tokens / total_tokens * 100) if total_tokens > 0 else 0\n",
    "    \n",
    "    print(f\"{name}: {entity_tokens:,}/{total_tokens:,} entity tokens ({entity_pct:.1f}%)\")\n",
    "    return entity_pct\n",
    "\n",
    "train_pct = analyze_labels(train_dataset, \"Train\")\n",
    "val_pct = analyze_labels(val_dataset, \"Val\")\n",
    "test_pct = analyze_labels(test_dataset, \"Test\")\n",
    "\n",
    "print(f\"\\n✓ Similar distributions across splits\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52fad94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 6 COMPLETE - SUMMARY\n",
      "================================================================================\n",
      "\n",
      "✓ Processed 6937 samples successfully\n",
      "✓ Saved to: ./layoutlm_dataset/\n",
      "✓ Total size: 4167.34 MB\n",
      "\n",
      "--- Dataset Statistics ---\n",
      "Train: 5549 samples (18.4% entities)\n",
      "Val: 694 samples (18.9% entities)\n",
      "Test: 694 samples (18.8% entities)\n",
      "\n",
      "--- Ready for Kaggle Upload ---\n",
      "Upload folder: ./layoutlm_dataset/\n",
      "  ├── train/ (3333.51 MB)\n",
      "  ├── val/ (416.92 MB)\n",
      "  └── test/ (416.92 MB)\n",
      "\n",
      "================================================================================\n",
      "✅ PHASE 6 SUCCESS - Ready for Phase 7!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# FINAL SUMMARY\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 6 COMPLETE - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n✓ Processed {len(labeled_data)} samples successfully\")\n",
    "print(f\"✓ Saved to: {output_dir}/\")\n",
    "print(f\"✓ Total size: {total_size:.2f} MB\")\n",
    "\n",
    "print(\"\\n--- Dataset Statistics ---\")\n",
    "print(f\"Train: {len(train_dataset)} samples ({train_pct:.1f}% entities)\")\n",
    "print(f\"Val: {len(val_dataset)} samples ({val_pct:.1f}% entities)\")\n",
    "print(f\"Test: {len(test_dataset)} samples ({test_pct:.1f}% entities)\")\n",
    "\n",
    "print(\"\\n--- Ready for Kaggle Upload ---\")\n",
    "print(f\"Upload folder: {output_dir}/\")\n",
    "print(f\"  ├── train/ ({train_size:.2f} MB)\")\n",
    "print(f\"  ├── val/ ({val_size:.2f} MB)\")\n",
    "print(f\"  └── test/ ({test_size:.2f} MB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ PHASE 6 SUCCESS - Ready for Phase 7!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4cd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
